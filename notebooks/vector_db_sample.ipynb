{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Vector databases"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In this tutorial, we will explore vector databases in greater detail. Previously, we focused on smaller examples at the sentence level. Now, we will examine longer texts. For this tutorial, we will utilize [Wikipedia](https://www.wikipedia.org/) as a resource. The [Wikipedia library](https://pypi.org/project/wikipedia/) allows easy access to and utilization of Wikipedia articles, making it an excellent choice for our purposes. \n",
    "\n",
    "Our primary focus will be implementing strategies for dividing the text as well as calculating and storing embeddings in the vector database. Thereafter, we will query the vector database.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Prerequisites"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In this tutorial, we re-use the PostgreSQL database Docker image we have presented in the *introduction_to_vector_databases* notebook. If you have already tested the showcase example and have not deleted the container *postgres-wier* with the database, you can start the Docker container as indicated below.\n",
    "\n",
    "\n",
    "\n",
    "<img src=\"https://i.ibb.co/DgCxJWdQ/docker-demo.png\" alt=\"docker-demo\" border=\"0\">\n",
    "\n",
    "\n",
    "\n",
    "Otherwise, follow next steps. \n",
    "\n",
    "First, prepare a file *database.sql*. The script will create a table with two rows:\n",
    "\n",
    "``` sql\n",
    "CREATE SCHEMA IF NOT EXISTS showcase;\n",
    "\n",
    "CREATE TABLE showcase.counters (\n",
    "    counter_id integer  NOT NULL,\n",
    "    value integer NOT NULL,\n",
    "    CONSTRAINT pk_counters PRIMARY KEY ( counter_id )\n",
    " );\n",
    "\n",
    "INSERT INTO showcase.counters VALUES (1,0), (2,0);\n",
    "```\n",
    "\n",
    "Go to an empty folder and save the script into a subfolder named *init-scripts*. Create another empty folder named *pgdata*.\n",
    "\n",
    "We run the docker container using the following command. The command will name the container *postgresql-wier*, set username and password, map database files to folder *./pgdata* and initialization scripts to *./init-scripts*, map port 5432 to host machine (i.e. localhost) and run image *pgvector:pg16* in a detached mode. \n",
    "\n",
    "``` \n",
    "docker run --name postgresql-wier \\\n",
    "    -e POSTGRES_PASSWORD=SecretPassword \\\n",
    "    -e POSTGRES_USER=user \\\n",
    "    -e POSTGRES_DB=wier \\\n",
    "    -v $PWD/pgdata:/var/lib/postgresql/data \\\n",
    "    -v $PWD/init-scripts:/docker-entrypoint-initdb.d \\\n",
    "    -p 5432:5432 \\\n",
    "    -d pgvector/pgvector:pg16\n",
    "```\n",
    "\n",
    "If you use Command Prompt on Windows, the equivalent of the above command is as follows:\n",
    "\n",
    "``` \n",
    "docker run --name postgresql-wier ^\n",
    "    -e POSTGRES_PASSWORD=SecretPassword ^\n",
    "    -e POSTGRES_USER=user ^\n",
    "    -e POSTGRES_DB=wier ^\n",
    "    -v \"%CD%\\pgdata:/var/lib/postgresql/data\" ^\n",
    "    -v \"%CD%\\init-scripts:/docker-entrypoint-initdb.d\" ^\n",
    "    -p 5432:5432 ^\n",
    "    -d pgvector/pgvector:pg16\n",
    "```\n",
    "\n",
    "To check the container's logs, run `docker logs -f postgresql-wier`.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Getting started\n",
    "\n",
    "First, install the necessary dependencies."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%pip install pgvector\n",
    "%pip install sentence_transformers\n",
    "%pip install numpy\n",
    "%pip install nltk\n",
    "%pip install wikipedia\n",
    "%pip install beautifulsoup4\n",
    "%pip install stanza"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "If you have already enabled the pgvector extension in the PostgreSQL database, you do not have to enable the pgvector extension again.  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pgvector.psycopg import register_vector\n",
    "import psycopg\n",
    "\n",
    "#connect to db\n",
    "conn = psycopg.connect(host=\"localhost\", dbname='wier', autocommit=True, password='SecretPassword', user='user')\n",
    "\n",
    "#enable `vector` extension if not already enabled\n",
    "conn.execute('CREATE EXTENSION IF NOT EXISTS vector')\n",
    "register_vector(conn)\n",
    "conn.close()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Retrieval of articles from Wikipedia"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We are now ready to begin our work. First, if the tables we need for this tutorial already exist, we will delete them. Then, we create the table showcase.wiki_articles with columns:\n",
    "- *id*: primary key\n",
    "- *topic*: topic of the Wikipedia page\n",
    "- *content*: content of the Wikipedia page for the topic "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#connect to the db\n",
    "conn = psycopg.connect(host=\"localhost\", dbname='wier', autocommit=True, password='SecretPassword', user='user')\n",
    "\n",
    "#delete tables from the db if they already exist\n",
    "conn.execute('DROP TABLE IF EXISTS showcase.wiki_chunks_sentences')\n",
    "conn.execute('DROP TABLE IF EXISTS showcase.wiki_chunks_fixed_length')\n",
    "conn.execute('DROP TABLE IF EXISTS showcase.wiki_articles')\n",
    "\n",
    "#create table wiki_articles with columns id, topic and content\n",
    "conn.execute('CREATE TABLE showcase.wiki_articles (id bigserial PRIMARY KEY, topic text, content text)')\n",
    "\n",
    "conn.close()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In the first step, we get some Wikipedia articles by means of the Wikipedia library for Python for the selected topics. For demonstration purposes, we use the Slovenian language. Replace \"sl\" with \"en\" in the setup to obtain articles in English."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import wikipediaapi\n",
    "\n",
    "# 1. Wikipedia API setup\n",
    "USER_AGENT = \"MySuperIEPSWikiBot/1.0\" #define user agent\n",
    "wiki_wiki = wikipediaapi.Wikipedia(user_agent=USER_AGENT, #user agent\n",
    "                                   language=\"sl\", #define the language. Replace \"sl\" with \"en\" to obtain articles in English.\n",
    "                                   extract_format=wikipediaapi.ExtractFormat.WIKI)#to get the the HTML content use .WIKI instead of .WIKI \n",
    "\n",
    "# 2. Define topics of the articles for retrieval\n",
    "topics = [\"Albert Einstein\", \"Taylor Swift\", \"Harry Potter\", \"J. K. Rowling\", \n",
    "          \"Magic Johnson\", \"Luka Dončić\", \"Peter Prevc\", \n",
    "          \"London\", \"Berlin\", \"Ljubljana\",  \n",
    "          \"Bitcoin\", \"Jupiter\", \"Twitter\" \n",
    "          ]\n",
    "\n",
    "# 3. Fetch Wikipedia articles and store them in the db\n",
    "conn = psycopg.connect(host=\"localhost\", dbname='wier', autocommit=True, password='SecretPassword', user='user')\n",
    "cur = conn.cursor() \n",
    "for topic in topics:\n",
    "    print('Fetching content of the Wikipedia article for the topic: ' + topic)\n",
    "    page = wiki_wiki.page(topic)\n",
    "    if page.exists():\n",
    "        print(\"URL of the page: \" + page.fullurl)\n",
    "        text = page.text # Get full content\n",
    "        cur.execute('INSERT INTO showcase.wiki_articles (topic, content) VALUES (%s, %s)', (topic, text))\n",
    "    else:\n",
    "        print(\"No page found for the topic \" + topic)\n",
    "cur.close()\n",
    "conn.close() "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Dividing the content, calculating and storing embeddings in vector database"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In the next step, you have to extract the relevant content from the web page. It is of utmost importance to consider which content is relevant to your selected domain when doing Programming Assignment 2 (PA2). Also of note is that you will likely deal with lengthy text, which you will have to clean and divide into smaller portions, as most models for calculating embeddings have limitations on the number of words they can process at once.\n",
    "\n",
    "As we have already downloaded the plain text of the articles, we skip this step of extracting relevant content and focus on dividing the text. There are [several approaches](https://www.analyticsvidhya.com/blog/2024/10/chunking-techniques-to-build-exceptional-rag-systems/) on how to divide text, e.g.:\n",
    "\n",
    "1. Divide text into fixed-size segments.\n",
    "2. Vary the sizes of text segments when dividing the text.\n",
    "3. Sliding window.\n",
    "4. ...\n",
    "\n",
    "For the PA2, you will have to select the most appropriate strategy for dividing text and list its advantages and disadvantages. It is often necessary to conduct some testing to determine what works best. Without a proper approach, we risk overlooking important information or providing incomplete or out-of-context retrieved segments. Some of the key factors to consider when dividing the text into smaller portions are the size of segments and context preservation.\n",
    "\n",
    "In this tutorial, we test two strategies. The first strategy divides the text into fixed-length segments, whereas the second strategy divides the text into text segments comprising sentences. The obtained text segment does not exceed the specified word count limit. We use the [nltk library](https://www.nltk.org/index.html)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import re\n",
    "import nltk\n",
    "import textwrap\n",
    "from nltk.tokenize import sent_tokenize\n",
    "\n",
    "# Download NLTK sentence tokenizer if not already installed\n",
    "nltk.download(\"punkt\")\n",
    "nltk.download(\"punkt_tab\")\n",
    "\n",
    "def chunk_fixed_length(text, chunk_size=50):\n",
    "    \"\"\"Fixed length chunking.\"\"\"\n",
    "    return [text[i:i+chunk_size] for i in range(0, len(text), chunk_size)]\n",
    "\n",
    "\n",
    "def chunk_segments(text, max_words=256):\n",
    "    \"\"\"Splits text into sentence-based chunks with a max word count limit.\"\"\"\n",
    "    sentences = sent_tokenize(text)  # Split into sentences\n",
    "    chunks, current_chunk = [], []\n",
    "    current_length = 0\n",
    "    \n",
    "    for sentence in sentences:\n",
    "        words = sentence.split()\n",
    "        if current_length + len(words) > max_words:\n",
    "            chunks.append(\" \".join(current_chunk))  # Save current chunk\n",
    "            current_chunk, current_length = [], 0  # Reset chunk\n",
    "        current_chunk.append(sentence)\n",
    "        current_length += len(words)\n",
    "    \n",
    "    if current_chunk:\n",
    "        chunks.append(\" \".join(current_chunk))  # Add last chunk\n",
    "\n",
    "    return chunks"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Below are two functions we use in this tutorial. The first one obtaines Wikipedia articles from the database, whereas the second function creates two tables (showcase.wiki_chunks_fixed_length, showcase.wiki_chunks_sentences) for storing embeddings using different strategies. Both tables have columns as follows:\n",
    "- *chunk_id*: primary key\n",
    "- *chunk*: text segment\n",
    "- *embeddings*: embedding for the text segment\n",
    "- *fk_pageid*: id of the page that includes the text segment and foreign key"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_wiki_articles():\n",
    "    \"\"\"Get Wikipedia articles from the table showcase.wiki_articles.\"\"\"\n",
    "\n",
    "    wiki_articles = [] \n",
    "\n",
    "    conn = psycopg.connect(host=\"localhost\", dbname='wier', autocommit=True, password='SecretPassword', user='user')\n",
    "    cur = conn.cursor()\n",
    "    cur.execute(\"SELECT id, topic, content FROM showcase.wiki_articles\")\n",
    "    for id, topic, content in cur.fetchall():\n",
    "        wiki_articles.append((id, topic, content))\n",
    "    cur.close()\n",
    "    conn.close()\n",
    "\n",
    "    return wiki_articles\n",
    "\n",
    "\n",
    "def create_tables():\n",
    "    \"\"\"Create tables showcase.wiki_chunks_fixed_length and showcase.wiki_chunks_sentences.\"\"\"\n",
    "    conn = psycopg.connect(host=\"localhost\", dbname='wier', autocommit=True, password='SecretPassword', user='user')\n",
    "    \n",
    "    conn.execute('DROP TABLE IF EXISTS showcase.wiki_chunks_fixed_length')\n",
    "    conn.execute('DROP TABLE IF EXISTS showcase.wiki_chunks_sentences')\n",
    "    \n",
    "    conn.execute('CREATE TABLE showcase.wiki_chunks_fixed_length (chunk_id bigserial PRIMARY KEY, chunk text, embedding vector(768), fk_pageid bigserial)')\n",
    "    conn.execute('CREATE TABLE showcase.wiki_chunks_sentences (chunk_id bigserial PRIMARY KEY, chunk text, embedding vector(768), fk_pageid bigserial)')\n",
    "    conn.execute('ALTER TABLE showcase.wiki_chunks_fixed_length ADD CONSTRAINT fk_pageid FOREIGN KEY ( fk_pageid ) REFERENCES showcase.wiki_articles( id ) ON DELETE RESTRICT;')\n",
    "    conn.execute('ALTER TABLE showcase.wiki_chunks_sentences ADD CONSTRAINT fk_pageid FOREIGN KEY ( fk_pageid ) REFERENCES showcase.wiki_articles( id ) ON DELETE RESTRICT;')\n",
    "\n",
    "    conn.close()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Next, we calculate embeddings using the two strategies we defined above and store them in the vector database. We use the embedding model LaBSE of the Sentence Transformer library to calculate embeddings. One page worth exploring for the model selection is the [Hugging Face web page](https://huggingface.co/models)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sentence_transformers import SentenceTransformer\n",
    "\n",
    "wiki_articles = get_wiki_articles()\n",
    "create_tables()\n",
    "model = SentenceTransformer('sentence-transformers/LaBSE')\n",
    "\n",
    "conn = psycopg.connect(host=\"localhost\", dbname='wier', autocommit=True, password='SecretPassword', user='user')\n",
    "cur = conn.cursor() \n",
    "\n",
    "for article in wiki_articles:\n",
    "    id = article[0]\n",
    "    topic = article[1]\n",
    "    wiki_content = article[2]\n",
    "\n",
    "    #segment wiki content\n",
    "    chunks1 = chunk_fixed_length(wiki_content)\n",
    "    chunks2 = chunk_segments(wiki_content)\n",
    "\n",
    "    #calculate embeddings for each chunk and store them into db\n",
    "    for single_chunk in chunks1:\n",
    "        #print(single_chunk)\n",
    "        embedding = model.encode(single_chunk).tolist()\n",
    "        cur.execute('INSERT INTO showcase.wiki_chunks_fixed_length (chunk, embedding, fk_pageid) VALUES (%s, %s, %s)', (single_chunk, embedding, id))\n",
    "\n",
    "    for single_chunk in chunks2:\n",
    "        embedding = model.encode(single_chunk).tolist()\n",
    "        cur.execute('INSERT INTO showcase.wiki_chunks_sentences (chunk, embedding, fk_pageid) VALUES (%s, %s, %s)', (single_chunk, embedding, id))\n",
    "\n",
    "cur.close()\n",
    "conn.close()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Querying "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "By default, *pgvector* performs an exact search when querying the database. Adding [index](https://github.com/pgvector/pgvector?tab=readme-ov-file#indexing) enables us to use approximate nearest neighbour (ANN) search. The *pgvector* supports two indexes, HSNW and IVFFlat. Compared to the IVFFlat, it achieves better performance. For this reason, it has been selected for this tutorial. When defining the HSNW index, you must add an index for each distance function you want to use for querying. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "conn = psycopg.connect(host=\"localhost\", dbname='wier', autocommit=True, password='SecretPassword', user='user')\n",
    "conn.execute('CREATE INDEX ON showcase.wiki_chunks_fixed_length USING hnsw (embedding vector_cosine_ops) WITH (m = 16, ef_construction = 64);')\n",
    "conn.execute('CREATE INDEX ON showcase.wiki_chunks_sentences USING hnsw (embedding vector_cosine_ops) WITH (m = 16, ef_construction = 64);')\n",
    "conn.close()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Below is the function *query_db_cosine* we use to retrieve the top 5 most similar sentences from a pgvector database based on the cosine distance."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#query using cosine distance\n",
    "def query_db_cosine(query, model_name, table_name):\n",
    "    \"\"\"\n",
    "    The query_db_cosine function retrieves the top 5 most similar sentences from a pgvector database based on cosine distance. \n",
    "    It uses a pre-trained SentenceTransformer model to encode the input query and then searches for the closest embeddings stored in the database.\n",
    "\n",
    "    Parameters\n",
    "    - query (str): The input text query to be searched.\n",
    "    - model_name (str): The name of the SentenceTransformer model to be used for encoding the query.\n",
    "    - table_name (str): The name of the table containing the stored sentence embeddings. Possible options are showcase.vector_demo and showcase.vector_demo2\n",
    "    \"\"\"\n",
    "    \n",
    "    #download the model\n",
    "    model = SentenceTransformer(model_name)\n",
    "\n",
    "    #calculate embedding for the query\n",
    "    query_embedding = model.encode(query).tolist()  \n",
    "\n",
    "    conn = psycopg.connect(host=\"localhost\", dbname='wier', autocommit=True, password='SecretPassword', user='user')\n",
    "    cur = conn.cursor() \n",
    "\n",
    "    # execute the query to fetch the top 5 most similar sentences based on cosine distance\n",
    "    result = cur.execute(\n",
    "        'SELECT chunk, 1 - (embedding <=> %s::vector) AS similarity '\n",
    "        'FROM ' + table_name + ' ORDER BY similarity DESC LIMIT 5',\n",
    "        (query_embedding,)  # pass the embedding twice, once for ordering and once for calculation\n",
    "    ).fetchall()\n",
    "    cur.close()\n",
    "    conn.close()\n",
    "    return result"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Next, we will try some examples."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Example 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "query = 'Kdo je Harry Potter?'  #Who is Harry Potter?\n",
    "model_name = 'sentence-transformers/LaBSE'\n",
    "table_name = 'showcase.wiki_chunks_fixed_length'\n",
    "\n",
    "query_db_cosine(query, model_name, table_name)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "query = 'Kdo je Harry Potter?' #Who is Harry Potter?\n",
    "model_name = 'sentence-transformers/LaBSE'\n",
    "table_name = 'showcase.wiki_chunks_sentences'\n",
    "\n",
    "query_db_cosine(query, model_name, table_name)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Example 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "query = 'glavno mesto Nemčije' #the capital of Germany\n",
    "model_name = 'sentence-transformers/LaBSE'\n",
    "table_name = 'showcase.wiki_chunks_fixed_length'\n",
    "query_db_cosine(query, model_name, table_name)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "query = 'glavno mesto Nemčije' #the capital of Germany\n",
    "model_name = 'sentence-transformers/LaBSE'\n",
    "table_name = 'showcase.wiki_chunks_sentences'\n",
    "query_db_cosine(query, model_name, table_name)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Example 3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "query = 'pop izvajalka' #pop artist\n",
    "model_name = 'sentence-transformers/LaBSE'\n",
    "table_name = 'showcase.wiki_chunks_fixed_length'\n",
    "query_db_cosine(query, model_name, table_name)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "query = 'pop izvajalka' #pop artist\n",
    "model_name = 'sentence-transformers/LaBSE'\n",
    "table_name = 'showcase.wiki_chunks_sentences'\n",
    "query_db_cosine(query, model_name, table_name)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Example 4"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "query = 'Taylor Swift' \n",
    "model_name = 'sentence-transformers/LaBSE'\n",
    "table_name = 'showcase.wiki_chunks_fixed_length'\n",
    "query_db_cosine(query, model_name, table_name)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "query = 'Taylor Swift' \n",
    "model_name = 'sentence-transformers/LaBSE'\n",
    "table_name = 'showcase.wiki_chunks_sentences'\n",
    "query_db_cosine(query, model_name, table_name)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Example 5"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "query = 'smučarski skakalec' #ski jumper\n",
    "model_name = 'sentence-transformers/LaBSE'\n",
    "table_name = 'showcase.wiki_chunks_fixed_length'\n",
    "query_db_cosine(query, model_name, table_name)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "query = 'smučarski skakalec' #ski jumper\n",
    "model_name = 'sentence-transformers/LaBSE'\n",
    "table_name = 'showcase.wiki_chunks_sentences'\n",
    "query_db_cosine(query, model_name, table_name)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Example 6"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "query = 'potpouuri' \n",
    "model_name = 'sentence-transformers/LaBSE'\n",
    "table_name = 'showcase.wiki_chunks_fixed_length'\n",
    "query_db_cosine(query, model_name, table_name)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "query = 'potpouuri'\n",
    "model_name = 'sentence-transformers/LaBSE'\n",
    "table_name = 'showcase.wiki_chunks_sentences'\n",
    "query_db_cosine(query, model_name, table_name)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
