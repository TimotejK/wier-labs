[["index.html", "Web Information Extraction and Retrieval About the course Lab work Programming assignments", " Web Information Extraction and Retrieval Timotej Knez, Melanija Vezočnik, prof. dr. Marko Bajec, Slavko Žitnik Thursday, 20. March, 2025 About the course This script includes instructions for the lab work for the Web Information Extraction and Retrieval course at the Faculty for computer and information science, University of Ljubljana. Lab work Please regularly check the Weekly plan and course announcements for possible changes. Attendance at labs is optional. However, it is mandatory to attend all programming assignments’ defences. Programming assignments In this project, students will build a Retrieval-Augmented Generation (RAG) chatbot for a specific domain, integrating web information extraction and retrieval with generative AI. The project will be developed across three programming assignments: Web Crawling and Information Gathering: Students will collect relevant domain-specific data by crawling websites and extracting useful content. Document Parsing and Knowledge Base Construction: Extracted data will be processed, cleaned, and structured into a knowledge base optimized for retrieval. Question Answering System Construction: A chatbot will be built using RAG techniques, combining information retrieval with a language model to generate accurate and context-aware responses. By the end of the project, students will have a functional chatbot capable of answering domain-specific queries using real-world data. More information available in the course Moodle classroom. Course projects (alternative to the programming assignments) If there is someone (or a group) that is interested in a specific field of information retrieval, we can arrange a custom assignment for her/him/them instead of working on the above three assignments. Such project can also be extended into a masters thesis. "],["PA1.html", "Chapter 1 Programming assignment 1 1.1 Introduction 1.2 Instructions 1.3 Basic tools 1.4 What to include in the report 1.5 What to submit", " Chapter 1 Programming assignment 1 1.1 Introduction The goal of this programming assignment is to build a standalone crawler that will crawl websites within your chosen domain. The crawler will roughly consist of the following components (Figure 1.1): HTTP downloader and renderer: To retrieve and render a web page. Data extractor: Minimal functionalities to extract images and hyperlinks. Duplicate detector: To detect already parsed pages. URL frontier: A list of URLs waiting to be parsed. Datastore: To store the data and additional metadata used by the crawler. Figure 1.1: Web crawler architecture. 1.2 Instructions Implement a web crawler that will crawl websites within your chosen domain. You can choose a programming language of your choice. Choose a domain you will be exploring in your project assignments. We have prepared some domains that could be interesting: University of Ljubljana (UL) and Faculty of Computer and Information Science (FRI) regulations: Create a system for answering quesitons about the study process at the University of Ljubljana. The goal is to answer questions like: “What are the requirements for enrolling in a higher grade”, “What is the class web information extraction and retrieval about?” or “What is the Code of Ethics at UL”. To achieve this, you should crawl the UL website and FRI website and create a database of information with special emphasis on regulatory documents. In this domain, the amount of documents will be smaller; however, you will need to extract information from PDF documents, which can be more challenging than extracting information from HTML pages. Slo-Tech: Extract discussions, trends, and insights by crawling Slo-Tech, a Slovenian online portal focused on the field of information technologies from a webpage slo-tech.com. Workaway: Crate a system for answering questions about global volunteering opportunities by crawling the platform Workaway that connects travellers with hosts worldwide. The goal is to extract publicly available information (e.g., host descriptions, FAQs) Med.Over.Net: Gather useful advice on a variety of topics by crawling Med.Over.Net. Med.Over.Net is a forum that contains discussions about health related issues. Slovenian legal domain: Crawl documents available on sodnapraksa.si. The website offers a large repository of legal documents structured in HTML format, which should help your chatbot answer legal questions. Other domains: If you would like to explore a different domain, let us know and we can determine the details. The crawler needs to be implemented with multiple workers that retrieve different web pages in parallel. The number of workers should be a parameter when starting the crawler. The frontier strategy needs to follow the preferential strategy. In the report explain how your strategy is implemented. Check and respect the robots.txt file for each domain if it exists. Correctly respect the commands User-agent, Allow, Disallow, Crawl-delay and Sitemap. Make sure to respect robots.txt as sites that define special crawling rules often contain spider traps. Also make sure that you follow ethics and do not send request to the same server more often than one request in 5 seconds (not only domain but also IP!). In a database store canonicalized URLs only! During crawling you need to detect duplicate web pages. The easiest solution is to check whether a web page with the same page content was already parsed (hint: you can extend the database with a hash, otherwise you need compare exact HTML code). If your crawler gets a URL from a frontier that has already been parsed, this is not treated as a duplicate. In such cases there is no need to re-crawl the page, just add a record into to the table link accordingly. BONUS POINTS: Deduplication using exact match is not efficient as some minor content can be different but two web pages can still be the same. Implement one of the Locality-sensitive hashing methods to find collisions and then apply Jaccard distance (e.g. using unigrams) to detect a possible duplicate. Also, select parameters for this method. Document your implementation and include an example of duplicate detection in the report. Note, you need to implement the method yourself to get bonus points. When your crawler fetches and renders a web page, do some simple parsing to detect images and next links. When parsing links, include links from href attributes and onclick Javascript events (e.g. location.href or document.location). Be careful to correctly extend the relative URLs before adding them to the frontier. Detect images on a web page only based on img tag, where the src attribute points to an image URL. BONUS POINTS: Implement a strategy for preferential crawling. Detect the relevance of each link to your domain and crawl more relevant links first. This will also help with your second and third tasks as you will have more relavant pages to work with. Donwload HTML content only (and PDF where required for the domain). List all other content (.doc, .docx, .ppt and .pptx) in the page_data table - there is no need to populate data field (i.e. binary content). In case you put a link into a frontier and identify content as a binary source, you can just set its page_type to BINARY. The same holds for the image data. In your crawler implementation you can use libraries that implement headless browsers but not libraries that already implement web crawler functionality. Therefore, some useful libraries that you can use are: HTML Cleaner HTML Parser JSoup Jaunt API HTTP Client Selenium phantomJS HTMLUnit etc. On the other hand, you MUST NOT use libraries like the following: Scrapy Apache Nutch crawler4j gecco Norconex HTTP Collector webmagic Webmuncher etc. To make sure that you correctly gather all the needed content placed into the DOM by Javascript, you should use headless browsers. Googlebot implements this as a two-step process or expects to retrieve dynamically built web page from an HTTP server. A nice session on crawling modern web sites built using JS frameworks, link parsing and image indexing was a part of Google IO 2018 and it is suggested for you to check it: Examples of enabling javascript in a web browser or not: Javascript enabled Javascript disabled In your implementation you must set the User-Agent field of your bot to fri-wier-NAME_OF_YOUR_GROUP. 1.2.1 Crawldb design Below there is a model of a crawldb database that your crawler needs to use. This is just a base model, which you MUST NOT change, but you can extend it with additional fields, tables, … that your crawler might need. You should use PostgreSQL database and create a schema using a prepared SQL script. Table site contains web site specific data. Each site can contain multiple web pages - table page. Populate all the fields accordingly when parsing. If a page is of type HTML, its content should be stored as a value within html_content attribute, otherwise (if crawler detects a binary file - e.g. .doc), html_content is set to NULL and a record in the page_data table is created. Available page type codes are HTML, BINARY, DUPLICATE and FRONTIER. The duplicate page should not have set the html_content value and should be linked to a duplicate version of a page. You can optionally use table page also as a current frontier queue storage. 1.3 Basic tools We propose to run the notebook using an Anaconda environment. Prepare the environment as follows: # Create and activate environment (activate it before each use) conda create -n wier python=3.9 conda activate wier # Install dependencies conda install selenium psycopg2 nb_conda requests conda install -c anaconda flask pyopenssl conda install -c conda-forge flask-httpauth # Run Jupyter notebook jupyter notebook During the lab session we will present basic tools for those who are not well experienced in Web scraping and database access: Jupyter notebook tutorial Web crawling - basic tools that introduces the basic tools to start working on the assignment. A showcase of server (Remote crawler database (server)) and client (Remote crawler database (client)) implementation in case you would like to run multiple crawlers (e.g. from each group member homes) and have the same crawler database. Jupiter notebook tutorial on Preferential crawling. Jupiter notebook tutorial on Vector databases. This introductory tutorial to vector databases is NOT applicable for PA1 1.4 What to include in the report The report should follow the standard structure. It should not exceed 2-3 pages. You can include extra pages if you need them for visualisations of the database or large tables. In the report include the following: All the specifics and decisions you make based on the instructions above and describe the implementation of your crawler. Document also parameters that are needed for your crawler, specifics, problems that you had during the development and solutions. For the sites that are given in the instructions’ seed list and also for the whole crawldb together (for both separately) report general statistics of crawldb (number of sites, number of web pages, number of duplicates, number of binary documents by type, number of images, average number of images per web page, …). Visualize links and include images into the report. If the network is too big, take only a portion of it or high-level representation (e.g. interconnectedness of specific domains). Use visualization libraries such as D3js, visjs, sigmajs or gephi. Describe how you implemented deduplication strategy with partial matching. Describe how you implemented a strategy for preferential crawling. Describe how you implemented a strategy for preferential crawling. 1.5 What to submit Only one of the group members should make a submission of the assignment in moodle. The submission should contain only a link to the repository that contains the following which you will use for all the submissions during the course: a file pa1/db a file pa1/report.pdf - PDF report. a file pa1/README.md - Short description of the project and instructions to install, set up and run the crawler. a folder pa1/crawler - Implementation of the crawler. NOTE: The database dump you submit should not contain images or binary data. Filename db should be of Custom export format that you can export directly using pgAdmin: The exported data file should not be larger than 100MB. If you get a larger file after exporting the database, please upload it to a cloud service like Google drive and include a text file with a link in your repository. For this assignment it is enough to retrieve data from 5.000 web pages in total (number of records in table page of type HTML from your domain). If there is less than 5000 pages available in your domain you should collect all relevant pages and describe in your report how you determined that there are no more useful pages. "],["PA2.html", "Chapter 2 Programming assignment 2 2.1 Introduction 2.2 Identifying and extracting information 2.3 Storing extracted information in a vector database 2.4 Information retrieval 2.5 What to submit", " Chapter 2 Programming assignment 2 2.1 Introduction The goal of this programming assignment is to extract useful information from the crawled websites and store it in a vector database for answering domain-specific questions. The assignment consists of three parts: Identifying and extracting important information Storing information to a vector database Retrieving information relevant to the query 2.2 Identifying and extracting information In this part of the assignment, your task is to identify meaningful content (such as article titles, product names, descriptions, prices, or publication dates) from HTML documents and extract it in a structured format. The goal is to prepare high-quality data that can later be stored in a vector database and queried effectively. You will explore two main techniques: XPath expressions and regular expressions. 2.2.1 Using XPath expressions One approach to navigate XML and HTML files is to use XPath expressions. XPath (XML Path Language) is a syntax for navigating through elements and attributes in an XML/HTML document tree. XPath allows you to: Select nodes or node-sets in an XML document Navigate based on element names, attribute values, and hierarchy Perform conditional searches using predicates Basic XPath Syntax: /html/body/div — selects a div element that is a child of body, which is a child of html //p — selects all &lt;p&gt; elements in the document //div[@class=\"content\"] — selects all &lt;div&gt; elements with a class attribute equal to “content” //a[@href] — selects all anchor tags with an href attribute Python Example Using XPath: from lxml import html html_content = &quot;&quot;&quot; &lt;html&gt; &lt;body&gt; &lt;div class=&quot;article&quot;&gt; &lt;h2&gt;Sample Article&lt;/h2&gt; &lt;p&gt;This is a test paragraph.&lt;/p&gt; &lt;/div&gt; &lt;/body&gt; &lt;/html&gt; &quot;&quot;&quot; tree = html.fromstring(html_content) title = tree.xpath(&#39;//div[@class=&quot;article&quot;]/h2/text()&#39;) print(title) # Output: [&#39;Sample Article&#39;] 2.2.2 Using regular expressions Another approach to content extraction is using regular expressions (regex). Regular expressions are powerful tools for pattern matching and text processing. While regex can be faster for small and well-structured tasks, it is more error-prone for messy or deeply nested HTML. Use it when you know the structure of the text you’re working with and want to quickly extract specific elements. Basic Regex Syntax: . — matches any character except newline * — matches 0 or more repetitions + — matches 1 or more repetitions ? — makes the preceding token optional [] — matches any one character in the brackets () — groups expressions \\d — matches any digit \\s — matches whitespace ^ — matches the start of a string $ — matches the end of a string Common Regex Examples and What They Match: Regular Expression Matches &lt;h2&gt;(.*?)&lt;/h2&gt; The text inside an &lt;h2&gt; tag href=\"(.*?)\" The value of an href attribute &lt;p&gt;(.*?)&lt;/p&gt; Paragraph content between &lt;p&gt; tags \\d{4}-\\d{2}-\\d{2} Dates in YYYY-MM-DD format \\$[0-9]+\\.[0-9]{2} Dollar amounts like $19.99 &lt;[^&gt;]+&gt; Any HTML tag You can test your regular expressions expressions at regexr.com. Python Example Using Regular Expressions: import re html_content = &quot;&quot;&quot; &lt;html&gt; &lt;body&gt; &lt;div class=&quot;article&quot;&gt; &lt;h2&gt;Sample Article&lt;/h2&gt; &lt;p&gt;Published on 2024-12-05&lt;/p&gt; &lt;p&gt;Price: $19.99&lt;/p&gt; &lt;a href=&quot;https://example.com&quot;&gt;Read more&lt;/a&gt; &lt;/div&gt; &lt;/body&gt; &lt;/html&gt; &quot;&quot;&quot; # Extract article title title = re.search(r&#39;&lt;h2&gt;(.*?)&lt;/h2&gt;&#39;, html_content) print(title.group(1)) # Output: Sample Article # Extract date date = re.search(r&#39;\\d{4}-\\d{2}-\\d{2}&#39;, html_content) print(date.group(0)) # Output: 2024-12-05 # Extract price price = re.search(r&#39;\\$[0-9]+\\.[0-9]{2}&#39;, html_content) print(price.group(0)) # Output: $19.99 # Extract link link = re.search(r&#39;href=&quot;(.*?)&quot;&#39;, html_content) print(link.group(1)) # Output: https://example.com Regular expressions can be concise and effective for small-scale, well-formatted content. However, for more complex HTML, tools like XPath (or HTML parsers like BeautifulSoup) are generally more reliable. 2.2.3 Your task You should use XPath and Regular expressions to filter the HTML websites you gathered in PA1. Prepare a plain text version of the website where you remove static elements like website headers, menus, footers etc. while keeping the page’s content. Split the page into multiple thematic sections based on your judgment. You can use paragraphs, sentences, page sections, or any other indicator, with the goal of each section being a full description of one topic. 2.3 Storing extracted information in a vector database The second step of the assignment is to store the extracted information in a vector database. For this, you will expand the database you created in PA1 with a new table that stores document segments and their vector representations. 2.3.1 Preparing the database Make sure you are using a pgvector database. Pgvector is a PostgreSQL extension that adds support for storing and indexing vector data. You can create a new pgvector database using the following command: docker run --name postgresql-wier \\ -e POSTGRES_PASSWORD=SecretPassword \\ -e POSTGRES_USER=user \\ -e POSTGRES_DB=wier \\ -v $PWD/pgdata:/var/lib/postgresql/data \\ -v $PWD/init-scripts:/docker-entrypoint-initdb.d \\ -p 5432:5432 \\ -d pgvector/pgvector:pg16 In your database, enable the vector extension using the statement: CREATE EXTENSION IF NOT EXISTS vector; Once you have the database set up, add a new column to the page table to store cleaned plain text content, and create a new table to store page segments and their vector embeddings. ALTER TABLE crawl_db.page ADD COLUMN cleaned_content TEXT; CREATE TABLE crawl_db.page_segment ( id serial NOT NULL, page_id integer, page_segment TEXT, embedding vector(768) ); ALTER TABLE crawl_db.page_segment ADD CONSTRAINT fk_page_page_segment FOREIGN KEY (page_id) REFERENCES crawl_db.page(id) ON DELETE RESTRICT; You are encouraged to extend the database schema with additional metadata fields that may be useful during retrieval — for example, the HTML tag, section title, heading level, or local context of a segment. If you do, describe these design decisions in your report. 2.3.2 Generate vector embeddings The next step is to generate vector embeddings for each page segment. Embeddings are fixed-length vector representations of text that capture its semantic meaning. You should experiment with different embedding models, such as: all-MiniLM-L6-v2 from sentence-transformers OpenAI’s text-embedding-ada-002 Hugging Face Transformers like distilbert-base-uncased Any domain-specific models relevant to your dataset (e.g. legal, medical) The goal is that similar queries and segments have similar embeddings — so that when you compare a query embedding to the segment embeddings, you retrieve the most relevant content. You can generate embeddings using Python libraries like sentence-transformers, transformers, or openai. For each segment, compute its embedding and store it in the embedding column of the page_segment table. 2.4 Information retrieval In this part of the assignment, you will create a simple program that takes a query (for example, a user question) and retrieves the most relevant text segments from your database using vector similarity search. You can use similarity operators provided by pgvector, such as: &lt;#&gt; Inner product (higher is more similar) &lt;=&gt; Cosine distance (lower is more similar) &lt;+&gt; L1 distance (lower is more similar) Your task is to experiment with: Different similarity metrics Different embedding models Including or excluding segment metadata in your embeddings (e.g., including the title or section tag) Chunking strategies for segmenting the page (fixed-length? by paragraph? by heading?) The goal is to find the best combination for your domain, so that given a query, your retriever consistently returns the most useful results. You should implement a small Python demo program that: Accepts a user query Computes its embedding Performs a similarity search over the segment embeddings in your database Returns and displays the top-k most relevant segments 2.5 What to submit Push your work into the same repository as you used for the first assignment. The repository must comply with the following structure: pa2/ ├── report-extraction.pdf # PDF report with description and evaluation ├── README.md # Setup instructions for running the code ├── implementation-extraction/ # Your implementation code (well documented) │ └── demo.py # A demo script that allows us to test the retriever ├── extraction-db/ # Your database including extracted segments and their embeddings 2.5.1 What to include in the report Your report should include the following: Information about how you performed website filtering How did you determine page the segments What embeddings did you decide to use, and why What similarity metric did you decide to use, and why Examples of queries and your retriever’s responses Limitations of your document retriever (e.g. examples of queries with bad responses) Make sure to also describe methods you tried but decided not to use. "],["404.html", "Page not found", " Page not found The page you requested cannot be found (perhaps it was moved or renamed). You may want to try searching to find the page's new location, or use the table of contents to find the page you are looking for. "]]
