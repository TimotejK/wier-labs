[
["index.html", "Web Information Extraction and Retrieval About the course Lab work Programming assignments Grading and rules", " Web Information Extraction and Retrieval assist. prof. dr. Slavko Žitnik and prof. dr. Marko Bajec Monday, 13. April, 2020 About the course This script includes instructions for the lab work for the Web Information Extraction and Retrieval course at the Faculty for computer and information science, University of Ljubljana. Lab work The lab work in this course is consultation-like, so we do not check attendance. The assistant is available during the scheduled times and you can select the time to ask questions in person, while you are also encouraged to use the course Slack channel. Programming assignments Programming assignment 1: Web crawling Submission deadline: March 27, 2020 @ 11:55pm Submission deadline: April 3, 2020 @ 11:55pm Programming assignment 2: Document parsing Submission deadline: April 24, 2020 @ 11:55pm Submission deadline: May 1, 2020 @ 11:55pm Programming assignment 3: Indexing &amp; querying Submission deadline: May 22, 2020 @ 11:55pm OPTIONAL assignment completion task: If there is someone (or a group) that is interested in a specific field of information retrieval, we can arrange a custom assignment for him (them) instead working on the above three assignment. An example of a project idea: * A large corpus acquisition, text preprocessing (vectorization) and analysis of vector-based information retrieval (e.g. milvus search engine). Grading and rules Each assigment is worth 100 points. Therefore, altogether 300 points. For late submission, there will be penalty of 10 points per each late day. It will be possible to submit up to 3 days after the assignment deadline. Lab work can be done in groups (preferably) of up to three students. Groups with two or a single member have the same pass conditions - they do not need to coordinate so much and are also more independent. As a result of an assignment, a group needs to create a private GitHub repository and add user szitnik to the repository with at least read credentials. All the submissions will be subject to plagiarism check. Groups that will copy the results or code from the others will be graded with zero points. After the submission of an assignment, all the submissions will be marked and each group will get a response. Randomly chosen groups will be selected to explain their solution and to show the execution in practice - each group will need to defend an assigment at least once during the course. Labs are mostly organized in a consultation-like manner. Assistant will be present at every lab time slot. If there is a no-show after 15min, he will be accessible in R2.58. Do not hesitate to reach out to the assistant or ask questions via Slack. "],
["PA1.html", "Chapter 1 Programming assignment 1 1.1 Introduction 1.2 Instructions 1.3 Basic tools 1.4 What to include in the report 1.5 What to submit 1.6 Grading schema", " Chapter 1 Programming assignment 1 1.1 Introduction The goal of this programming assignment is to build a standalone crawler that will crawl only .gov.si web sites. The crawler will roughly consist of the following components (Figure 1.1): HTTP downloader and renderer: To retrieve and render a web page. Data extractor: Minimal functionalities to extract images and hyperlinks. Duplicate detector: To detect already parsed pages. URL frontier: A list of URLs waiting to be parsed. Datastore: To store the data and additional metadata used by the crawler. Figure 1.1: Web crawler architecture. 1.2 Instructions Implement a web crawler that will crawl only *.gov.si web sites. You can choose a programming language of your choice. The initial seed URLs should be: gov.si, evem.gov.si, e-uprava.gov.si and e-prostor.gov.si. For the above given domains only (not other domains), Apache Nutch needs the following time: HTTP retrieval without rendering: cca. 60min Retrieved around 7.000 pages at the level 14. HTTP retrieval with rendering (Selenium - HTMLUnit): cca. 230min - 290min Retrieved around 3.000 pages. All the parameters are set to default settings (5s between requests to the same server, …). Selenium/HTMLUnit protocol retrieves significantly less web pages due to problems in parsing evem.gov.si and e-uprava.gov.si web sites. The crawler needs to be implemented with multiple workers that retrieve different web pages in parallel. The number of workers should be a parameter when starting the crawler. The frontier strategy needs to follow the breadth-first strategy. In the report explain how is your strategy implemented. For each domain respect the robots.txt file if it exists. Correctly respect the commands User-agent, Allow, Disallow, Crawl-delay and Sitemap. Make sure to respect robots.txt as sites that define special crawling rules often contain spider traps. Also make sure that you follow ethics and do not send request to the same server more often than one request in 5 seconds (not only domain but also IP!). During crawling you need to detect duplicate web pages. Check URLs that you already parsed and URLs that you have in the frontier if a duplicate exist. Make sure that you work with canonicalized URLs only! If you do not find duplicates by a URL, check if a web page with the same content was parsed already (extend the database with a hash or compare exact HTML code). BONUS POINTS (10 points): Deduplication using exact match is not efficient as some minor content can be different but two web pages can still be the same. Implement one of the Locality-sensitive hashing methods to find collisions and then apply Jaccard distance (e.g. using unigrams) to detect a possible duplicate. Also, select parameters for this method. Document your implementation and include an example of duplicate detection in the report. When your crawler fetches and renders a web page, do some simple parsing to detect images and next links. When parsing links, include links from href attributes and onclick Javascript events (e.g. location.href or document.location). Be careful to correctly extend the relative URLs before adding them to the frontier. Detect images on a web page only based on img tag, where the src attribute points to an image URL. Donwload HTML content only. List all other content (.pdf, .doc, .docx, .ppt and .pptx) in the page_data table - there is no need to populate data field. In case you put a link into a frontier and identify content as a binary source, you can just set its page_type to BINARY. The same holds for the image data. In your crawler implementation you can use libraries that implement headless browsers but not libraries that already implement web crawler functionality. Therefore, some useful libraries that you can use are: HTML Cleaner HTML Parser JSoup Jaunt API HTTP Client Selenium phantomJS HTMLUnit etc. On the other hand, you MUST NOT use libraries like the following: Scrapy Apache Nutch crawler4j gecco Norconex HTTP Collector webmagic Webmuncher etc. To make sure that you correctly gather all the needed content placed into the DOM by Javascript, you should use headless browsers. Googlebot implements this as a two-step process or expects to retrieve dynamically built web page from an HTTP server. A nice session on crawling modern web sites built using JS frameworks, link parsing and image indexing was a part of Google IO 2018 and it is suggested for you to check it: Examples of enabling javascript in a web browser or not: Javascript enabled Javascript disabled In your implementation you must set the User-Agent field of your bot to fri-ieps-NAME_OF_YOUR_GROUP. 1.2.1 Crawldb design Below there is a model of a crawldb database that your crawler needs to use. This is just a base model, which you MUST NOT change, but you can extend it with additional fields, tables, … that your crawler might need. You should use PostgreSQL database and create a schema using a prepared SQL script. Table site contains web site specific data. Each site can contain multiple web pages - table page. Populate all the fields accordingly when parsing. If a page is of type HTML, its content should be stored as a value within html_content attribute, otherwise (if crawler detects a binary file - e.g. .doc), html_content is set to NULL and a record in the page_data table is created. Available page type codes are HTML, BINARY, DUPLICATE and FRONTIER. The duplicate page should not have set the html_content value and should be linked to a duplicate version of a page. You can optionally use table page also as a current frontier queue storage. 1.3 Basic tools During the lab session we will present basic tools for those who are not well experienced in Web scraping and database access. We will follow the Jupyter notebook tutorial Web crawling - basic tools. We propose to run the notebook using an Anaconda environment. Prepare the environment as follows: conda create -n wier python=3.6 conda activate wier conda install nb_conda # to automatically run jupyter notebook in the current conda environment jupyter notebook # run this command in the folder with the Jupyter notebook 1.4 What to include in the report The report should follow the standard structure. It must not exceed 2 pages. In the report include the following: All the specifics and decisions you make based on the instructions above and describe the implementation of your crawler. Document also parameters that are needed for your crawler, specifics, problems that you had during the development and solutions. For the sites that are given in the instructions’ seed list and also for the whole crawldb together (for both separately) report general statistics of crawldb (number of sites, number of web pages, number of duplicates, number of binary documents by type, number of images, average number of images per web page, …). Visualize links and include images into the report. If the network is too big, take only a portion of it or high-level representation (e.g. interconnectedness of specific domains). Use visualization libraries such as D3js, visjs, sigmajs or gephi. 1.5 What to submit Only one of the group members should make a submission of the assignment in moodle. The submission should contain only a link to the repository that contains the following: a file db a file report.pdf - PDF report. a file README.md - Short description of the project and instructions to install, set up and run the crawler. a folder crawler - Implementation of the crawler. NOTE: The database dump you submit should not contain images or binary data. Filename db should be of Custom export format that you can export directly using pgAdmin: The exported data file should not be larger than 100MB. For this assignment it is enough to retrieve data from up 50.000 web pages in total (number of records in table page of type HTML from .gov.si domains). 1.6 Grading schema All the submissions will be manually graded by the assistant. Also plagiarism check will be run across all the submissions. Grading will begin after the last late submission day. The submission time will be selected as the last commit time in the repository. The maximum score of 100 (+10 bonus points) will consist of the following: Points Item 20 Database dump check (selected web pages, robots.txt compliance, rough number of web pages match) 30 Crawler implementation details (multiple workers, BFS frontier, robots.txt and sitemap check, binary files saving, non-crawler libraries usage, javascript rendering) 10 Duplicate detection (URL canonicalization, content matching) 10 Duplicate detection (BONUS) 10 Image and link detection (image tags and saving, HTML and JS) 20 Retrieved pages analysis (statistics and visualization; justification of retrieved pages wrt. crawler running time) 10 Submission compliance (report of work and issues description, readme instructions, source code availability - 0 points for the whole project if not available) Selected groups will need to defend their work during the lab hours. If a group does not agree with their achieved score, it will be able to “negotiate”/defend their programming assignment submission. "],
["PA2.html", "Chapter 2 Programming assignment 2 2.1 Introduction 2.2 Instructions 2.3 What to include in the report 2.4 What to submit 2.5 Grading schema", " Chapter 2 Programming assignment 2 2.1 Introduction The goal of this programming assignment is to implement three different approaches for the structured data extraction from the Web: Using regular expressions Using XPath RoadRunner-like implementation 2.2 Instructions In a compressed file there are four web pages - two per Web site (Overstock, Rtvslo.si). In Figure 2.1 we define the names of the data items for a data record you will need to extract from Overstock sample web sites. The provided sample Web sites contain a list of data records, which all need to be processed. In Figure 2.2 we also provide the names of the data items for the second sample web sites. In the provided Rtvslo.si example, there is only one data record per sample page. Figure 2.1: Overstock.com Web page sample. Figure 2.2: RtvSlo.si Web page sample. Similarly to given Web pages above, find your own two similar Web pages (can be either list pages or detail pages from other domains than given ones) and define the names of the data items that you will extract. The two Web pages must contain some same data item types with different values and a list of data items of different lengths. For each of the three types of the pages, implement the following: Data extraction using regular expressions only. Data extraction using XPath only. Generation of extraction rules using automatic Web extraction algorithm. Input HTML files to these method should be pre-rendered. The implementation must contain a file named run-extraction.py that can be run from command-line to test all the methods. The file should take one parameter as input - type of extracting algorithm (A, B, C). The script will be called as follows: python run-extraction.py A. Output should be written to standard output for all the Web pages. The code must be run using python 3.6 and all the needed libraries must be listed in the README.md file. If you use any other language or Python version, you need to provide your own Docker image to run the scripts. 2.2.1 Regular expressions implementation For each given web page type implement a separate function that will take HTML code as input. The method should output extracted data in a JSON structured format to a standard output. Each data item must be directly extracted using a regular expression. Each data item must be extracted using only one regular expression and not more. But you can extract multiple data items using one regular expression (this might help you when extracting optional items in list pages). 2.2.2 XPath implementation For each given web page type implement a separate function that will take HTML code as input. The method should output extracted data in a JSON structured format to a standard output. Each data item must be directly extracted using an XPath expression. If the extracted value should be further processed, use regular expressions or other techniques to normalize them. 2.2.3 Automatic Web extraction algorithm implementation Implement a method that will take two HTML web pages of the same type as input. The method should output a human-readable wrapper that could be used to extract data from a given type of web pages. The wrapper can be represented as union-free regular expression or any other format, based on which someone could implement a web data extractor. Web data extractor implementation is not needed for the purposes of this assignment. You must not use code of the existing implementations but you can check it out or read related papers to get more in-depth knowledge if interested (not necessary for the purposes of this programming assignment). You can implement an algorithm of your choice. We propose to implement one of the following: RoadRunner-like implementation: Follow the implementation guidelines presented at the lectures. Apart from the guidelines you can introduce various heuristics (e.g. taking tag attributes into account, additional rules, …) that may help you with the wrapper generation. Also, you can implement some preprocessing steps prior to running the algorithm. It is expected, that the generated wrapper may miss some of the data items and may include other unrelevant data items in the output. You should comment or identify, which data items have not been recognized using the algorithm. Full implementation of the RoadRunner algorithm proposed in the literature is available online along with some examples. Webstemmer-like implementation: Follow the implementation guidelines of the Webstemmer algorithm and try to implement it on your own. Other implementations: Try your own heuristics, ideas, … Search the Internet for other existing automatic extraction techniques and implement an appropriate algorithm. 2.3 What to include in the report The report should follow the standard structure. In the report include the following: Description of the two selected web pages and identification of data items and data records (similarly as in the instructions above). Regular expressions implementation: a list of regular expressions that extract data from all the pages. XPath implementation: a list of XPath expressions that extract data from all the pages. Automatic Web extraction implementation: Pseudocode of your algorithm implementation and describtion of its implementation. Explain all the rules or heuristics. Justify each inclusion of a rule or heuristics. Also cite the source of the idea if you re-implemented some feature from somewhere else (e.g. literature, full RoadRunner implementation). Provide an output wrapper for each web page pair (i.e. three outputs). Each output should not exceed one A4 paper. 2.4 What to submit Push your work into the same repository as you used for the first assignment. Structure the repository must comply with the following structure: a file /report-extraction.pdf - PDF report. a file /README.md - Short description of the project and instructions to install, set up and run all the methods. a folder /implementation-extraction - Implementation of the methods along with run-extraction.py. CAUTION: Use relative paths to files in your repository and take into account that script will be run within the implementation-extraction folder. a folder /input-extraction - All the three types of web pages that implemented methods can consume. 2.5 Grading schema All the submissions will be graded semi-automatically by the assistant. Plagiarism check will be run across all the submissions. Grading will begin after the last late submission day. The submission time will be selected as the last commit time in the repository. The maximum score of 100 will consist of the following: Points Item 10 Selection of two similar Web pages 20 Regular expressions and XPath implementation 30 Automatic Web extraction implementation and description 30 Reproducibility of your work (10 points per method) 10 Submission compliance (report, readme instructions, repository structure) Selected groups will need to defend their work during the lab hours. If a group does not agree with their achieved score, it will be able to “negotiate”/defend their programming assignment submission. "]
]
