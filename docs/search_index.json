[["index.html", "Web Information Extraction and Retrieval About the course Lab work Programming assignments", " Web Information Extraction and Retrieval assist. prof. dr. Slavko Žitnik and prof. dr. Marko Bajec Wednesday, 13. April, 2022 About the course This script includes instructions for the lab work for the Web Information Extraction and Retrieval course at the Faculty for computer and information science, University of Ljubljana. Lab work The lab work in this course is consultation-like, so we do not check attendance. The assistant is available during the scheduled times and you can select the time to ask questions in person, while you are also encouraged to use the course Discord channel. Programming assignments Programming assignment 1: Web crawling Programming assignment 2: Document parsing Programming assignment 3: Indexing &amp; querying More information available in the course Moodle classroom. Course projects (alternative to the programming assignments) If there is someone (or a group) that is interested in a specific field of information retrieval, we can arrange a custom assignment for her/him/them instead working on the above three assignments. Such project can also be extended into a masters thesis. Proposed projects: Slovenian support for Apache Solr: Apache Solr is an open-source search engine and widely adopted. It allows for rich customization and definition of own filters, tokenizers, processes, etc. The idea of the project is to implement an Apache Solr plugin that would integrate CLASSLA library - e.g., lemmatization, tokenization, named entity recognition. Expected project deliverables are (a) implemented plugin in a source code repository with documentation, (b) written report, and (c) practical presentation of your work. Information retrieval systems evaluation: Some information retrieval systems have been developed into rich systems that require some knowledge and configuration to create a useful search engine. The idea of the project is that you are given crawled data from PA1 and then you use an information retrieval system to create a Web-based search engine. The selection of a search engine must be agreed with the assistant - e.g. milvus search engine). Expected project deliverables are (a) Docker-based implementation of a search engine with documentation, (b) Web-based functions for searching and documents insert (bulk), and (c) written report. "],["PA1.html", "Chapter 1 Programming assignment 1 1.1 Introduction 1.2 Instructions 1.3 Basic tools 1.4 What to include in the report 1.5 What to submit", " Chapter 1 Programming assignment 1 1.1 Introduction The goal of this programming assignment is to build a standalone crawler that will crawl only .gov.si web sites. The crawler will roughly consist of the following components (Figure 1.1): HTTP downloader and renderer: To retrieve and render a web page. Data extractor: Minimal functionalities to extract images and hyperlinks. Duplicate detector: To detect already parsed pages. URL frontier: A list of URLs waiting to be parsed. Datastore: To store the data and additional metadata used by the crawler. Figure 1.1: Web crawler architecture. 1.2 Instructions Implement a web crawler that will crawl only *.gov.si web sites. You can choose a programming language of your choice. The initial seed URLs should be: gov.si, evem.gov.si, e-uprava.gov.si and e-prostor.gov.si. For the above given domains only (not other domains), Apache Nutch needs the following time: HTTP retrieval without rendering: cca. 60min Retrieved around 7.000 pages at the level 14. HTTP retrieval with rendering (Selenium - HTMLUnit): cca. 230min - 290min Retrieved around 3.000 pages. All the parameters are set to default settings (5s between requests to the same server, …). Selenium/HTMLUnit protocol retrieves significantly less web pages due to problems in parsing evem.gov.si and e-uprava.gov.si web sites. The crawler needs to be implemented with multiple workers that retrieve different web pages in parallel. The number of workers should be a parameter when starting the crawler. The frontier strategy needs to follow the breadth-first strategy. In the report explain how is your strategy implemented. Check and respect the robots.txt file for each domain if it exists. Correctly respect the commands User-agent, Allow, Disallow, Crawl-delay and Sitemap. Make sure to respect robots.txt as sites that define special crawling rules often contain spider traps. Also make sure that you follow ethics and do not send request to the same server more often than one request in 5 seconds (not only domain but also IP!). In a database store canonicalized URLs only! During crawling you need to detect duplicate web pages. The easiest solution is to check whether a web page with the same page content was already parsed (hint: you can extend the database with a hash, otherwise you need compare exact HTML code). If your crawler gets a URL from a frontier that has already been parsed, this is not treated as a duplicate. In such cases there is no need to re-crawl the page, just add a record into to the table link accordingly. BONUS POINTS (10 points): Deduplication using exact match is not efficient as some minor content can be different but two web pages can still be the same. Implement one of the Locality-sensitive hashing methods to find collisions and then apply Jaccard distance (e.g. using unigrams) to detect a possible duplicate. Also, select parameters for this method. Document your implementation and include an example of duplicate detection in the report. Note, you need to implement the method yourself to get bonus points. When your crawler fetches and renders a web page, do some simple parsing to detect images and next links. When parsing links, include links from href attributes and onclick Javascript events (e.g. location.href or document.location). Be careful to correctly extend the relative URLs before adding them to the frontier. Detect images on a web page only based on img tag, where the src attribute points to an image URL. Donwload HTML content only. List all other content (.pdf, .doc, .docx, .ppt and .pptx) in the page_data table - there is no need to populate data field (i.e. binary content). In case you put a link into a frontier and identify content as a binary source, you can just set its page_type to BINARY. The same holds for the image data. In your crawler implementation you can use libraries that implement headless browsers but not libraries that already implement web crawler functionality. Therefore, some useful libraries that you can use are: HTML Cleaner HTML Parser JSoup Jaunt API HTTP Client Selenium phantomJS HTMLUnit etc. On the other hand, you MUST NOT use libraries like the following: Scrapy Apache Nutch crawler4j gecco Norconex HTTP Collector webmagic Webmuncher etc. To make sure that you correctly gather all the needed content placed into the DOM by Javascript, you should use headless browsers. Googlebot implements this as a two-step process or expects to retrieve dynamically built web page from an HTTP server. A nice session on crawling modern web sites built using JS frameworks, link parsing and image indexing was a part of Google IO 2018 and it is suggested for you to check it: Examples of enabling javascript in a web browser or not: Javascript enabled Javascript disabled In your implementation you must set the User-Agent field of your bot to fri-wier-NAME_OF_YOUR_GROUP. 1.2.1 Crawldb design Below there is a model of a crawldb database that your crawler needs to use. This is just a base model, which you MUST NOT change, but you can extend it with additional fields, tables, … that your crawler might need. You should use PostgreSQL database and create a schema using a prepared SQL script. Table site contains web site specific data. Each site can contain multiple web pages - table page. Populate all the fields accordingly when parsing. If a page is of type HTML, its content should be stored as a value within html_content attribute, otherwise (if crawler detects a binary file - e.g. .doc), html_content is set to NULL and a record in the page_data table is created. Available page type codes are HTML, BINARY, DUPLICATE and FRONTIER. The duplicate page should not have set the html_content value and should be linked to a duplicate version of a page. You can optionally use table page also as a current frontier queue storage. 1.3 Basic tools We propose to run the notebook using an Anaconda environment. Prepare the environment as follows: # Create and activate environment (activate it before each use) conda create -n wier python=3.6 conda activate wier # Install dependencies conda install selenium psycopg2 nb_conda requests conda install -c anaconda flask pyopenssl conda install -c conda-forge flask-httpauth # Run Jupyter notebook jupyter notebook During the lab session we will present basic tools for those who are not well experienced in Web scraping and database access: Jupyter notebook tutorial Web crawling - basic tools that introduces the basic tools to start working on the assignment. A showcase of server (Remote crawler database (server)) and client (Remote crawler database (client)) implementation in case you would like to run multiple crawlers (e.g. from each group member homes) and have the same crawler database. 1.4 What to include in the report The report should follow the standard structure. It must not exceed 2 pages. In the report include the following: All the specifics and decisions you make based on the instructions above and describe the implementation of your crawler. Document also parameters that are needed for your crawler, specifics, problems that you had during the development and solutions. For the sites that are given in the instructions’ seed list and also for the whole crawldb together (for both separately) report general statistics of crawldb (number of sites, number of web pages, number of duplicates, number of binary documents by type, number of images, average number of images per web page, …). Visualize links and include images into the report. If the network is too big, take only a portion of it or high-level representation (e.g. interconnectedness of specific domains). Use visualization libraries such as D3js, visjs, sigmajs or gephi. 1.5 What to submit Only one of the group members should make a submission of the assignment in moodle. The submission should contain only a link to the repository that contains the following which you will use for all the submissions during the course: a file pa1/db a file pa1/report.pdf - PDF report. a file pa1/README.md - Short description of the project and instructions to install, set up and run the crawler. a folder pa1/crawler - Implementation of the crawler. NOTE: The database dump you submit should not contain images or binary data. Filename db should be of Custom export format that you can export directly using pgAdmin: The exported data file should not be larger than 100MB. For this assignment it is enough to retrieve data from up 50.000 web pages in total (number of records in table page of type HTML from .gov.si domains). "],["PA2.html", "Chapter 2 Programming assignment 2 2.1 Introduction 2.2 Instructions 2.3 What to include in the report 2.4 What to submit", " Chapter 2 Programming assignment 2 2.1 Introduction The goal of this programming assignment is to implement three different approaches for the structured data extraction from the Web: Using regular expressions Using XPath RoadRunner-like Wrapper implementation or other automatic content extraction 2.2 Instructions In a compressed file there are four Web pages - two per Web site (Overstock, Rtvslo.si). In Figure 2.1 we define the names of the data items for a data record you will need to extract from Overstock sample Web sites. The provided sample Web sites contain a list of data records, which all need to be processed. In Figure 2.2 we also provide the names of the data items for the second sample Web sites. In the provided Rtvslo.si example, there is only one data record per sample page. Figure 2.1: Overstock.com Web page sample. Figure 2.2: RtvSlo.si Web page sample. Similarly to given Web pages above, find your own example of two similar Web pages (can be either list pages or detail pages from other domains than given ones) and define the names of the data items that you will extract. The two Web pages must contain some same data item types with different values and a list of data items of different lengths. For each of the three types of the pages, implement the following: Data extraction using regular expressions only. Data extraction using XPath only. Generation of extraction rules using automatic Web extraction algorithm. Input HTML files to these method should be pre-rendered. The implementation must contain a file named run-extraction.py that can be run from command-line to test all the methods. The file should take one parameter as input - type of extracting algorithm (A, B, C). The script will be called as follows: python run-extraction.py A. Output should be written to standard output for all the Web pages. Instructions to run the code must be listed in the README.md file. 2.2.1 Regular expressions implementation For each given Web page type implement a separate function that will take HTML code as input. The method should output extracted data in a JSON structured format to a standard output. Each data item must be directly extracted using a single regular expression. But you can extract multiple data items using one regular expression (this might help you when extracting optional items in list pages). 2.2.2 XPath implementation For each given Web page type implement a separate function that will take HTML code as input. The method should output extracted data in a JSON structured format to a standard output. Each data item must be directly extracted using an XPath expression. If the extracted value should be further processed, use regular expressions or other techniques to normalize them (for text only!). 2.2.3 Automatic Web extraction algorithm implementation Implement a method that will take two HTML Web pages of the same type as input. The method should output a human-readable wrapper that could be used to extract data from a given type of Web pages. The wrapper can be represented as union-free regular expression or any other format, based on which someone could implement a Web data extractor. Web data extractor implementation is not needed for the purposes of this assignment. You must not use code of the existing implementations but you can check it out or read related papers to get more in-depth knowledge if interested (not necessary for the purposes of this programming assignment). You can implement an algorithm of your choice. We propose to implement one of the following: RoadRunner-like implementation: Follow the implementation guidelines presented at the lectures. Apart from the guidelines you can introduce various heuristics (e.g. taking tag attributes into account, additional rules, …) that may help you with the wrapper generation. Also, you can implement some preprocessing steps prior to running the algorithm. It is expected, that the generated wrapper may miss some of the data items and may include other unrelevant data items in the output. You should comment or identify, which data items have not been recognized using the algorithm. Full (official) implementation of the RoadRunner algorithm proposed in the literature is available online along with some examples. Check also other descriptions, e.g. paper 1 or paper 2. Webstemmer-like implementation: Follow the implementation guidelines of the Webstemmer algorithm and try to implement it on your own. Other implementations (for this option consult with the assistant): Search the Internet for other existing automatic extraction techniques and implement an appropriate algorithm, such as Automatic Web Content Extraction by Combination of Learning and Grouping, Wu et al., WWW 2015, Automatic Extraction of Informative Blocks from Webpages, Debnath et al., SAC 2005 or Automatic data extraction of Websites using data path matching and alignment, Chu et al., 2015. You must not use libraries such as jusText, trafilatura, readability, Newspaper3k, BoilerPy3, draget, Goose3 or news-please. In case you are interested in such libraries, you can re-implement one of them for the Slovene language (the work can be then extended to a MSc. thesis). 2.3 What to include in the report The report should follow the standard structure. In the report include the following (max 2 pages + outputs): Description of the two selected Web pages and identification of data items and data records (similarly as in the instructions above). Regular expressions implementation: a list of regular expressions that extract data from all the pages. XPath implementation: a list of XPath expressions that extract data from all the pages. Automatic Web extraction implementation: Pseudocode of your algorithm implementation and describtion of its implementation. Explain all the rules or heuristics. Justify each inclusion of a rule or heuristics. Also cite the source of the idea if you re-implemented some feature from somewhere else (e.g. literature, full RoadRunner implementation). Provide an output wrapper for each Web page pair (i.e. three outputs). Each output should not exceed one A4 paper. 2.4 What to submit Push your work into the same repository as you used for the first assignment. Structure the repository must comply with the following structure: a file pa2/report-extraction.pdf - PDF report. a file pa2/README.md - Short description of the project and instructions to install, set up and run all the methods. a folder pa2/implementation-extraction - Implementation of the methods along with run-extraction.py. CAUTION: Use relative paths to files in your repository and take into account that script will be run within the implementation-extraction folder. a folder pa2/input-extraction - All the three types of Web pages that implemented methods can consume. "],["404.html", "Page not found", " Page not found The page you requested cannot be found (perhaps it was moved or renamed). You may want to try searching to find the page's new location, or use the table of contents to find the page you are looking for. "]]
