<!DOCTYPE html>
<html lang="" xml:lang="">
<head>

  <meta charset="utf-8" />
  <meta http-equiv="X-UA-Compatible" content="IE=edge" />
  <title>Chapter 1 Programming assignment 1 | Web Information Extraction and Retrieval</title>
  <meta name="description" content="Instructions for the Web Information Extraction and Retrieval course labs" />
  <meta name="generator" content="bookdown 0.29 and GitBook 2.6.7" />

  <meta property="og:title" content="Chapter 1 Programming assignment 1 | Web Information Extraction and Retrieval" />
  <meta property="og:type" content="book" />
  
  <meta property="og:description" content="Instructions for the Web Information Extraction and Retrieval course labs" />
  

  <meta name="twitter:card" content="summary" />
  <meta name="twitter:title" content="Chapter 1 Programming assignment 1 | Web Information Extraction and Retrieval" />
  
  <meta name="twitter:description" content="Instructions for the Web Information Extraction and Retrieval course labs" />
  

<meta name="author" content="assist. prof. dr. Slavko Žitnik and prof. dr. Marko Bajec" />



  <meta name="viewport" content="width=device-width, initial-scale=1" />
  <meta name="apple-mobile-web-app-capable" content="yes" />
  <meta name="apple-mobile-web-app-status-bar-style" content="black" />
  
  
<link rel="prev" href="index.html"/>
<link rel="next" href="PA2.html"/>
<script src="libs/jquery-3.6.0/jquery-3.6.0.min.js"></script>
<script src="https://cdn.jsdelivr.net/npm/fuse.js@6.4.6/dist/fuse.min.js"></script>
<link href="libs/gitbook-2.6.7/css/style.css" rel="stylesheet" />
<link href="libs/gitbook-2.6.7/css/plugin-table.css" rel="stylesheet" />
<link href="libs/gitbook-2.6.7/css/plugin-bookdown.css" rel="stylesheet" />
<link href="libs/gitbook-2.6.7/css/plugin-highlight.css" rel="stylesheet" />
<link href="libs/gitbook-2.6.7/css/plugin-search.css" rel="stylesheet" />
<link href="libs/gitbook-2.6.7/css/plugin-fontsettings.css" rel="stylesheet" />
<link href="libs/gitbook-2.6.7/css/plugin-clipboard.css" rel="stylesheet" />








<link href="libs/anchor-sections-1.1.0/anchor-sections.css" rel="stylesheet" />
<link href="libs/anchor-sections-1.1.0/anchor-sections-hash.css" rel="stylesheet" />
<script src="libs/anchor-sections-1.1.0/anchor-sections.js"></script>


<style type="text/css">
pre > code.sourceCode { white-space: pre; position: relative; }
pre > code.sourceCode > span { display: inline-block; line-height: 1.25; }
pre > code.sourceCode > span:empty { height: 1.2em; }
.sourceCode { overflow: visible; }
code.sourceCode > span { color: inherit; text-decoration: inherit; }
pre.sourceCode { margin: 0; }
@media screen {
div.sourceCode { overflow: auto; }
}
@media print {
pre > code.sourceCode { white-space: pre-wrap; }
pre > code.sourceCode > span { text-indent: -5em; padding-left: 5em; }
}
pre.numberSource code
  { counter-reset: source-line 0; }
pre.numberSource code > span
  { position: relative; left: -4em; counter-increment: source-line; }
pre.numberSource code > span > a:first-child::before
  { content: counter(source-line);
    position: relative; left: -1em; text-align: right; vertical-align: baseline;
    border: none; display: inline-block;
    -webkit-touch-callout: none; -webkit-user-select: none;
    -khtml-user-select: none; -moz-user-select: none;
    -ms-user-select: none; user-select: none;
    padding: 0 4px; width: 4em;
    color: #aaaaaa;
  }
pre.numberSource { margin-left: 3em; border-left: 1px solid #aaaaaa;  padding-left: 4px; }
div.sourceCode
  {   }
@media screen {
pre > code.sourceCode > span > a:first-child::before { text-decoration: underline; }
}
code span.al { color: #ff0000; font-weight: bold; } /* Alert */
code span.an { color: #60a0b0; font-weight: bold; font-style: italic; } /* Annotation */
code span.at { color: #7d9029; } /* Attribute */
code span.bn { color: #40a070; } /* BaseN */
code span.bu { } /* BuiltIn */
code span.cf { color: #007020; font-weight: bold; } /* ControlFlow */
code span.ch { color: #4070a0; } /* Char */
code span.cn { color: #880000; } /* Constant */
code span.co { color: #60a0b0; font-style: italic; } /* Comment */
code span.cv { color: #60a0b0; font-weight: bold; font-style: italic; } /* CommentVar */
code span.do { color: #ba2121; font-style: italic; } /* Documentation */
code span.dt { color: #902000; } /* DataType */
code span.dv { color: #40a070; } /* DecVal */
code span.er { color: #ff0000; font-weight: bold; } /* Error */
code span.ex { } /* Extension */
code span.fl { color: #40a070; } /* Float */
code span.fu { color: #06287e; } /* Function */
code span.im { } /* Import */
code span.in { color: #60a0b0; font-weight: bold; font-style: italic; } /* Information */
code span.kw { color: #007020; font-weight: bold; } /* Keyword */
code span.op { color: #666666; } /* Operator */
code span.ot { color: #007020; } /* Other */
code span.pp { color: #bc7a00; } /* Preprocessor */
code span.sc { color: #4070a0; } /* SpecialChar */
code span.ss { color: #bb6688; } /* SpecialString */
code span.st { color: #4070a0; } /* String */
code span.va { color: #19177c; } /* Variable */
code span.vs { color: #4070a0; } /* VerbatimString */
code span.wa { color: #60a0b0; font-weight: bold; font-style: italic; } /* Warning */
</style>


<link rel="stylesheet" href="style.css" type="text/css" />
</head>

<body>



  <div class="book without-animation with-summary font-size-2 font-family-1" data-basepath=".">

    <div class="book-summary">
      <nav role="navigation">

<ul class="summary">
<li><a href="./index.html">Web Information Extraction and Retrieval</a></li>

<li class="divider"></li>
<li class="chapter" data-level="" data-path="index.html"><a href="index.html"><i class="fa fa-check"></i>About the course</a>
<ul>
<li class="chapter" data-level="" data-path="index.html"><a href="index.html#lab-work"><i class="fa fa-check"></i>Lab work</a></li>
<li class="chapter" data-level="" data-path="index.html"><a href="index.html#programming-assignments"><i class="fa fa-check"></i>Programming assignments</a>
<ul>
<li class="chapter" data-level="" data-path="index.html"><a href="index.html#course-projects-alternative-to-the-programming-assignments"><i class="fa fa-check"></i>Course projects (alternative to the programming assignments)</a></li>
</ul></li>
</ul></li>
<li class="chapter" data-level="1" data-path="PA1.html"><a href="PA1.html"><i class="fa fa-check"></i><b>1</b> Programming assignment 1</a>
<ul>
<li class="chapter" data-level="1.1" data-path="PA1.html"><a href="PA1.html#introduction"><i class="fa fa-check"></i><b>1.1</b> Introduction</a></li>
<li class="chapter" data-level="1.2" data-path="PA1.html"><a href="PA1.html#instructions"><i class="fa fa-check"></i><b>1.2</b> Instructions</a>
<ul>
<li class="chapter" data-level="1.2.1" data-path="PA1.html"><a href="PA1.html#crawldb-design"><i class="fa fa-check"></i><b>1.2.1</b> Crawldb design</a></li>
</ul></li>
<li class="chapter" data-level="1.3" data-path="PA1.html"><a href="PA1.html#basic-tools"><i class="fa fa-check"></i><b>1.3</b> Basic tools</a></li>
<li class="chapter" data-level="1.4" data-path="PA1.html"><a href="PA1.html#what-to-include-in-the-report"><i class="fa fa-check"></i><b>1.4</b> What to include in the report</a></li>
<li class="chapter" data-level="1.5" data-path="PA1.html"><a href="PA1.html#what-to-submit"><i class="fa fa-check"></i><b>1.5</b> What to submit</a></li>
</ul></li>
<li class="chapter" data-level="2" data-path="PA2.html"><a href="PA2.html"><i class="fa fa-check"></i><b>2</b> Programming assignment 2</a>
<ul>
<li class="chapter" data-level="2.1" data-path="PA2.html"><a href="PA2.html#introduction-1"><i class="fa fa-check"></i><b>2.1</b> Introduction</a></li>
<li class="chapter" data-level="2.2" data-path="PA2.html"><a href="PA2.html#instructions-1"><i class="fa fa-check"></i><b>2.2</b> Instructions</a>
<ul>
<li class="chapter" data-level="2.2.1" data-path="PA2.html"><a href="PA2.html#regular-expressions-implementation"><i class="fa fa-check"></i><b>2.2.1</b> Regular expressions implementation</a></li>
<li class="chapter" data-level="2.2.2" data-path="PA2.html"><a href="PA2.html#xpath-implementation"><i class="fa fa-check"></i><b>2.2.2</b> XPath implementation</a></li>
<li class="chapter" data-level="2.2.3" data-path="PA2.html"><a href="PA2.html#automatic-web-extraction-algorithm-implementation"><i class="fa fa-check"></i><b>2.2.3</b> Automatic Web extraction algorithm implementation</a></li>
</ul></li>
<li class="chapter" data-level="2.3" data-path="PA2.html"><a href="PA2.html#what-to-include-in-the-report-1"><i class="fa fa-check"></i><b>2.3</b> What to include in the report</a></li>
<li class="chapter" data-level="2.4" data-path="PA2.html"><a href="PA2.html#what-to-submit-1"><i class="fa fa-check"></i><b>2.4</b> What to submit</a></li>
</ul></li>
<li class="chapter" data-level="3" data-path="PA3.html"><a href="PA3.html"><i class="fa fa-check"></i><b>3</b> Programming assignment 3</a>
<ul>
<li class="chapter" data-level="3.1" data-path="PA3.html"><a href="PA3.html#introduction-2"><i class="fa fa-check"></i><b>3.1</b> Introduction</a></li>
<li class="chapter" data-level="3.2" data-path="PA3.html"><a href="PA3.html#instructions-2"><i class="fa fa-check"></i><b>3.2</b> Instructions</a>
<ul>
<li class="chapter" data-level="3.2.1" data-path="PA3.html"><a href="PA3.html#data-processing-and-indexing"><i class="fa fa-check"></i><b>3.2.1</b> Data processing and indexing</a></li>
<li class="chapter" data-level="3.2.2" data-path="PA3.html"><a href="PA3.html#data-retrieval"><i class="fa fa-check"></i><b>3.2.2</b> Data retrieval</a></li>
</ul></li>
<li class="chapter" data-level="3.3" data-path="PA3.html"><a href="PA3.html#what-to-include-in-the-report-2"><i class="fa fa-check"></i><b>3.3</b> What to include in the report</a></li>
<li class="chapter" data-level="3.4" data-path="PA3.html"><a href="PA3.html#what-to-submit-2"><i class="fa fa-check"></i><b>3.4</b> What to submit</a></li>
</ul></li>
<li class="divider"></li>
<li><a href="https://ucilnica.fri.uni-lj.si">Moodle classroom</a></li>

</ul>

      </nav>
    </div>

    <div class="book-body">
      <div class="body-inner">
        <div class="book-header" role="navigation">
          <h1>
            <i class="fa fa-circle-o-notch fa-spin"></i><a href="./">Web Information Extraction and Retrieval</a>
          </h1>
        </div>

        <div class="page-wrapper" tabindex="-1" role="main">
          <div class="page-inner">

            <section class="normal" id="section-">
<div id="PA1" class="section level1 hasAnchor" number="1">
<h1><span class="header-section-number">Chapter 1</span> Programming assignment 1<a href="PA1.html#PA1" class="anchor-section" aria-label="Anchor link to header"></a></h1>
<p><img src="img/book/spider-01.png" /></p>
<div id="introduction" class="section level2 hasAnchor" number="1.1">
<h2><span class="header-section-number">1.1</span> Introduction<a href="PA1.html#introduction" class="anchor-section" aria-label="Anchor link to header"></a></h2>
<p>The goal of this programming assignment is to build a standalone crawler that will crawl only <em>.gov.si</em> web sites. The crawler will roughly consist of the following components (Figure <a href="PA1.html#fig:crawlerArchitecture">1.1</a>):</p>
<ul>
<li>HTTP downloader and renderer: To retrieve and render a web page.</li>
<li>Data extractor: Minimal functionalities to extract images and hyperlinks.</li>
<li>Duplicate detector: To detect already parsed pages.</li>
<li>URL frontier: A list of URLs waiting to be parsed.</li>
<li>Datastore: To store the data and additional metadata used by the crawler.</li>
</ul>
<div class="figure" style="text-align: center"><span style="display:block;" id="fig:crawlerArchitecture"></span>
<img src="img/pa1/crawler.png" alt="Web crawler architecture." width="500" />
<p class="caption">
Figure 1.1: Web crawler architecture.
</p>
</div>
</div>
<div id="instructions" class="section level2 hasAnchor" number="1.2">
<h2><span class="header-section-number">1.2</span> Instructions<a href="PA1.html#instructions" class="anchor-section" aria-label="Anchor link to header"></a></h2>
<p>Implement a web crawler that will crawl only <em>*.gov.si</em> web sites. You can choose a programming language of your choice. The initial seed URLs should be:</p>
<ul>
<li><em>gov.si</em>,</li>
<li><em>evem.gov.si</em>,</li>
<li><em>e-uprava.gov.si</em> and</li>
<li><em>e-prostor.gov.si</em>.</li>
</ul>
<blockquote>
<p>For the above given domains only (not other domains), <a href="https://nutch.apache.org/">Apache Nutch</a> needs the following time:</p>
<ul>
<li>HTTP retrieval without rendering:
<ul>
<li>cca. 60min</li>
<li>Retrieved around 7.000 pages at the level 14.</li>
</ul></li>
<li>HTTP retrieval with rendering (Selenium - HTMLUnit):
<ul>
<li>cca. 230min - 290min</li>
<li>Retrieved around 3.000 pages.</li>
</ul></li>
</ul>
<p>All the parameters are set to default settings (5s between requests to the same server, …). Selenium/HTMLUnit protocol retrieves significantly less web pages due to problems in parsing <em>evem.gov.si</em> and <em>e-uprava.gov.si</em> web sites.</p>
</blockquote>
<p>The crawler needs to be implemented with multiple workers that retrieve different web pages in parallel. The number of workers should be a parameter when starting the crawler. The frontier strategy needs to follow the breadth-first strategy. In the report explain how is your strategy implemented.</p>
<p>Check and respect the <a href="https://en.wikipedia.org/wiki/Robots_exclusion_standard"><em>robots.txt</em></a> file for each domain if it exists. Correctly respect the commands <em>User-agent</em>, <em>Allow</em>, <em>Disallow</em>, <em>Crawl-delay</em> and <em>Sitemap</em>. Make sure to respect <em>robots.txt</em> as sites that define special crawling rules often contain <a href="https://en.wikipedia.org/wiki/Spider_trap">spider traps</a>. Also make sure that you follow ethics and do not send request to the same server more often than one request in 5 seconds (not only domain but also IP!).</p>
<p>In a database store canonicalized URLs only!</p>
<p>During crawling you need to detect duplicate web pages. The easiest solution is to check whether a web page with the same page content was already parsed (hint: you can extend the database with a hash, otherwise you need compare exact HTML code). If your crawler gets a URL from a frontier that has already been parsed, this is not treated as a duplicate. In such cases there is no need to re-crawl the page, just add a record into to the table <em>link</em> accordingly.</p>
<ul>
<li>BONUS POINTS (10 points): Deduplication using exact match is not efficient as some minor content can be different but two web pages can still be the same. Implement one of the <a href="https://en.wikipedia.org/wiki/Locality-sensitive_hashing">Locality-sensitive hashing</a> methods to find collisions and then apply Jaccard distance (e.g. using unigrams) to detect a possible duplicate. Also, select parameters for this method. Document your implementation and include an example of duplicate detection in the report. Note, you need to implement the method yourself to get bonus points.</li>
</ul>
<p>When your crawler fetches and renders a web page, do some simple parsing to detect images and next links.</p>
<ul>
<li>When parsing links, include links from <em>href</em> attributes and <em>onclick</em> Javascript events (e.g. <em>location.href</em> or <em>document.location</em>). Be careful to correctly extend the relative URLs before adding them to the frontier.</li>
<li>Detect images on a web page only based on <em>img</em> tag, where the <em>src</em> attribute points to an image URL.</li>
</ul>
<p>Donwload HTML content only. List all other content (<em>.pdf</em>, <em>.doc</em>, <em>.docx</em>, <em>.ppt</em> and <em>.pptx</em>) in the <em>page_data</em> table - there is no need to populate <em>data</em> field (i.e. binary content). In case you put a link into a frontier and identify content as a binary source, you can just set its <em>page_type</em> to <em>BINARY</em>. The same holds for the image data.</p>
<p>In your crawler implementation you can use libraries that implement headless browsers but not libraries that already implement web crawler functionality. Therefore, some useful libraries that you can use are:</p>
<ul>
<li><a href="http://htmlcleaner.sourceforge.net/">HTML Cleaner</a></li>
<li><a href="http://htmlparser.sourceforge.net/">HTML Parser</a></li>
<li><a href="https://jsoup.org/">JSoup</a></li>
<li><a href="https://jaunt-api.com/">Jaunt API</a></li>
<li><a href="http://hc.apache.org/">HTTP Client</a></li>
<li><a href="https://www.seleniumhq.org/">Selenium</a></li>
<li><a href="http://phantomjs.org/">phantomJS</a></li>
<li><a href="http://htmlunit.sourceforge.net/">HTMLUnit</a></li>
<li>etc.</li>
</ul>
<p>On the other hand, you <strong>MUST NOT</strong> use libraries like the following:</p>
<ul>
<li><a href="https://scrapy.org/">Scrapy</a></li>
<li><a href="https://nutch.apache.org/">Apache Nutch</a></li>
<li><a href="https://github.com/yasserg/crawler4j">crawler4j</a></li>
<li><a href="https://github.com/xtuhcy/gecco">gecco</a></li>
<li><a href="https://www.norconex.com/collectors/collector-http/">Norconex HTTP Collector</a></li>
<li><a href="https://github.com/code4craft/webmagic">webmagic</a></li>
<li><a href="https://github.com/dadepo/Webmuncher">Webmuncher</a></li>
<li>etc.</li>
</ul>
<p>To make sure that you correctly gather all the needed content placed into the DOM by Javascript, you should use headless browsers. Googlebot implements this as a two-step process or expects to retrieve dynamically built web page from an HTTP server. A nice session on crawling modern web sites built using JS frameworks, link parsing and image indexing was a part of Google IO 2018 and it is suggested for you to check it:</p>
<center>
<iframe width="560" height="315" src="https://www.youtube.com/embed/PFwUbgvpdaQ" frameborder="0" allow="accelerometer; autoplay; encrypted-media; gyroscope; picture-in-picture" allowfullscreen>
</iframe>
</center>
<!--
Useful links
 https://www.elephate.com/blog/javascript-seo-experiment/ 
 https://searchengineland.com/tested-googlebot-crawls-javascript-heres-learned-220157
 https://webmasters.googleblog.com/
-->
<p>Examples of enabling javascript in a web browser or not:</p>
<table class="additionalSources">
<tr>
<td>
<center>
<b>Javascript enabled</b>
</center>
</td>
<td>
<center>
<b>Javascript disabled</b>
</center>
</td>
</tr>
<tr>
<td>
<img src="img/pa1/eUprava_JS.png" />
</td>
<td>
<img src="img/pa1/eUprava_noJS.png" />
</td>
</tr>
<tr>
<td>
<img src="img/pa1/evem_JS.png" />
</td>
<td>
<img src="img/pa1/evem_noJS.png" />
</td>
</tr>
</table>
<p>In your implementation you must set the <em>User-Agent</em> field of your bot to <em>fri-wier-NAME_OF_YOUR_GROUP</em>.</p>
<div id="crawldb-design" class="section level3 hasAnchor" number="1.2.1">
<h3><span class="header-section-number">1.2.1</span> Crawldb design<a href="PA1.html#crawldb-design" class="anchor-section" aria-label="Anchor link to header"></a></h3>
<p>Below there is a model of a crawldb database that your crawler needs to use. This is just a base model, which you <strong>MUST NOT</strong> change, but you can extend it with additional fields, tables, … that your crawler might need. You should use PostgreSQL database and create a schema using a <a href="data/pa1/crawldb.sql">prepared SQL script</a>.</p>
<center>
<iframe width="879" height="814" src="data/pa1/crawldb.html" frameborder="0" scrolling="no">
</iframe>
</center>
<p>Table <em>site</em> contains web site specific data. Each site can contain multiple web pages - table <em>page</em>. Populate all the fields accordingly when parsing. If a page is of type HTML, its content should be stored as a value within <em>html_content</em> attribute, otherwise (if crawler detects a binary file - e.g. .doc), <em>html_content</em> is set to <em>NULL</em> and a record in the <em>page_data</em> table is created. Available page type codes are <em>HTML</em>, <em>BINARY</em>, <em>DUPLICATE</em> and <em>FRONTIER</em>. The duplicate page should not have set the <em>html_content</em> value and should be linked to a duplicate version of a page.</p>
<p>You can optionally use table <em>page</em> also as a current frontier queue storage.</p>
</div>
</div>
<div id="basic-tools" class="section level2 hasAnchor" number="1.3">
<h2><span class="header-section-number">1.3</span> Basic tools<a href="PA1.html#basic-tools" class="anchor-section" aria-label="Anchor link to header"></a></h2>
<p>We propose to run the notebook using an Anaconda environment. Prepare the environment as follows:</p>
<div class="sourceCode" id="cb1"><pre class="sourceCode bash"><code class="sourceCode bash"><span id="cb1-1"><a href="PA1.html#cb1-1" aria-hidden="true" tabindex="-1"></a><span class="co"># Create and activate environment (activate it before each use)</span></span>
<span id="cb1-2"><a href="PA1.html#cb1-2" aria-hidden="true" tabindex="-1"></a><span class="ex">conda</span> create <span class="at">-n</span> wier python=3.9</span>
<span id="cb1-3"><a href="PA1.html#cb1-3" aria-hidden="true" tabindex="-1"></a><span class="ex">conda</span> activate wier</span>
<span id="cb1-4"><a href="PA1.html#cb1-4" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb1-5"><a href="PA1.html#cb1-5" aria-hidden="true" tabindex="-1"></a><span class="co"># Install dependencies</span></span>
<span id="cb1-6"><a href="PA1.html#cb1-6" aria-hidden="true" tabindex="-1"></a><span class="ex">conda</span> install selenium psycopg2 nb_conda requests</span>
<span id="cb1-7"><a href="PA1.html#cb1-7" aria-hidden="true" tabindex="-1"></a><span class="ex">conda</span> install <span class="at">-c</span> anaconda flask pyopenssl</span>
<span id="cb1-8"><a href="PA1.html#cb1-8" aria-hidden="true" tabindex="-1"></a><span class="ex">conda</span> install <span class="at">-c</span> conda-forge flask-httpauth</span>
<span id="cb1-9"><a href="PA1.html#cb1-9" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb1-10"><a href="PA1.html#cb1-10" aria-hidden="true" tabindex="-1"></a><span class="co"># Run Jupyter notebook</span></span>
<span id="cb1-11"><a href="PA1.html#cb1-11" aria-hidden="true" tabindex="-1"></a><span class="ex">jupyter</span> notebook </span></code></pre></div>
<p>During the lab session we will present basic tools for those who are not well experienced in Web scraping and database access:</p>
<ul>
<li>Jupyter notebook tutorial <a href="notebooks/Web%20crawling%20-%20basic%20tools.ipynb">Web crawling - basic tools</a> that introduces the basic tools to start working on the assignment.</li>
<li>A showcase of server (<a href="notebooks/Remote%20crawler%20database%20(server).ipynb">Remote crawler database (server)</a>) and client (<a href="notebooks/Remote%20crawler%20database%20(client).ipynb">Remote crawler database (client)</a>) implementation in case you would like to run multiple crawlers (e.g. from each group member homes) and have the same crawler database.</li>
</ul>
</div>
<div id="what-to-include-in-the-report" class="section level2 hasAnchor" number="1.4">
<h2><span class="header-section-number">1.4</span> What to include in the report<a href="PA1.html#what-to-include-in-the-report" class="anchor-section" aria-label="Anchor link to header"></a></h2>
<p>The report should follow the <a href="https://fri.uni-lj.si/sl/napotki-za-pisanje-porocila">standard structure</a>. It must not exceed 2 pages.</p>
<p>In the report include the following:</p>
<ul>
<li>All the specifics and decisions you make based on the instructions above and describe the implementation of your crawler.</li>
<li>Document also parameters that are needed for your crawler, specifics, problems that you had during the development and solutions.</li>
<li>For the sites that are given in the instructions’ seed list and also for the whole crawldb together (for both separately) report general statistics of crawldb (number of sites, number of web pages, number of duplicates, number of binary documents by type, number of images, average number of images per web page, …).</li>
<li>Visualize links and include images into the report. If the network is too big, take only a portion of it or high-level representation (e.g. interconnectedness of specific domains). Use visualization libraries such as <a href="https://d3js.org/">D3js</a>, <a href="http://visjs.org/">visjs</a>, <a href="http://sigmajs.org/">sigmajs</a> or <a href="https://gephi.org/">gephi</a>.</li>
</ul>
</div>
<div id="what-to-submit" class="section level2 hasAnchor" number="1.5">
<h2><span class="header-section-number">1.5</span> What to submit<a href="PA1.html#what-to-submit" class="anchor-section" aria-label="Anchor link to header"></a></h2>
<p>Only one of the group members should make a submission of the assignment in moodle. The submission should contain only a link to the repository that contains the following which you will use for all the submissions during the course:</p>
<ul>
<li>a file <em>pa1/db</em></li>
<li>a file <em>pa1/report.pdf</em> - PDF report.</li>
<li>a file <em>pa1/README.md</em> - Short description of the project and instructions to install, set up and run the crawler.</li>
<li>a folder <em>pa1/crawler</em> - Implementation of the crawler.</li>
</ul>
<blockquote>
<p><strong>NOTE:</strong> The database dump you submit should not contain images or binary data. Filename <em>db</em> should be of <strong>Custom</strong> export format that you can export directly using pgAdmin:</p>
<center>
<img src="img/pa1/Export1.png" />
</center>
<center>
<img src="img/pa1/Export2.png" />
</center>
<p>The exported data file should not be larger than 100MB.</p>
<p>For this assignment it is enough to retrieve data from up 50.000 web pages in total (number of records in table <em>page</em> of type HTML from <em>.gov.si</em> domains).</p>
</blockquote>
<!--
## Grading schema

All the submissions will be manually graded by the assistant. Also, plagiarism check will be run across all the submissions. Grading will begin after the last late submission day. The submission time will be selected as the last commit time in the repository. 

The maximum score of 100 (+10 bonus points) will consist of the following:

Points | Item
------ | ----
20 | Database dump check (selected web pages, robots.txt compliance, rough number of web pages match)
30 | Crawler implementation details (multiple workers, BFS frontier, robots.txt and sitemap check, binary files saving, non-crawler libraries usage, javascript rendering)
10 | Duplicate detection (URL canonicalization, content matching)
10 | Duplicate detection (BONUS)
10 | Image and link detection (image tags and saving, HTML and JS)
20 | Retrieved pages analysis (statistics and visualization; justification of retrieved pages wrt. crawler running time)
10 | Submission compliance (report of work and issues description, readme instructions, source code availability - 0 points for the whole project if not available)

Selected groups will need to defend their work during the lab hours. If a group does not agree with their achieved score, it will be able to "negotiate"/defend their programming assignment submission.
-->

</div>
</div>
            </section>

          </div>
        </div>
      </div>
<a href="index.html" class="navigation navigation-prev " aria-label="Previous page"><i class="fa fa-angle-left"></i></a>
<a href="PA2.html" class="navigation navigation-next " aria-label="Next page"><i class="fa fa-angle-right"></i></a>
    </div>
  </div>
<script src="libs/gitbook-2.6.7/js/app.min.js"></script>
<script src="libs/gitbook-2.6.7/js/clipboard.min.js"></script>
<script src="libs/gitbook-2.6.7/js/plugin-search.js"></script>
<script src="libs/gitbook-2.6.7/js/plugin-sharing.js"></script>
<script src="libs/gitbook-2.6.7/js/plugin-fontsettings.js"></script>
<script src="libs/gitbook-2.6.7/js/plugin-bookdown.js"></script>
<script src="libs/gitbook-2.6.7/js/jquery.highlight.js"></script>
<script src="libs/gitbook-2.6.7/js/plugin-clipboard.js"></script>
<script>
gitbook.require(["gitbook"], function(gitbook) {
gitbook.start({
"sharing": {
"github": false,
"facebook": false,
"twitter": false,
"linkedin": false,
"weibo": false,
"instapaper": false,
"vk": false,
"whatsapp": false,
"all": {}
},
"fontsettings": {
"theme": "white",
"family": "sans",
"size": 1
},
"edit": {
"link": null,
"text": null
},
"history": {
"link": null,
"text": null
},
"view": {
"link": null,
"text": null
},
"download": null,
"search": {
"engine": "fuse",
"options": null
},
"toc": {
"collapse": "section",
"scroll_highlight": true
},
"toolbar": {
"position": "fixed"
}
});
});
</script>

</body>

</html>
