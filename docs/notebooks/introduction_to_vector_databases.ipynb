{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Introduction to vector databases"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Vector search"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Similar words"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We have already established that words are represented as high-dimensional vectors in the vector space known as embeddings. Words are transformed into embeddings by using embedding techniques, such as BERT. Also of note is that similar words tend to have vectors (embeddings) that are positioned nearby in the vector space.\n",
    "\n",
    "Without the loss of generality, let's suppose we have a vector space in two dimensions, where words are represented as two-dimensional points. The point (2, 3) denotes the word sunny while the point (3, 4) denotes the word sunlit. These two words have similar meanings. Hence, their vectors are located nearby in the vector space. On the other hand, the point (-2, -3.5) represents the word dark. Due to having the opposite meaning of sunny, the vector representing the word dark is situated apart from the vectors sunny and sunlit.\n",
    "\n",
    "<img src=\"https://i.ibb.co/1J84S8SX/untitled2.jpg\" alt=\"untitled2\" border=\"0\">\n",
    "\n",
    "Moreover, a query can be mapped into high-dimensional vectors, and a match can be found in the vector space. In vector query execution, we aim to discover similar vectors to identify the most relevant candidates for our search results. If the vector content is indexed, the type of index used guides the search for relevant matches, which may be either exhaustive or focused on near neighbours. Also of note is that the latter facilitates faster processing. Once we pinpoint our candidates, we score the results using similarity metrics that measure the strength of the match.\n",
    "\n",
    "Some popular algorithms used in vector search include KNN (K-Nearest Neighbours) and ANN (Approximate Nearest Neighbours). Both KNN and ANN are based on distance measurements to find the nearest neighbours. KNN finds the exact nearest neighbours, while ANN finds the approximate nearest neighbours. ANN can significantly improve search efficiency when compared to KNN when processing vast amounts of data, especially when dealing with high-dimensional vectors.\n",
    "\n",
    "In the scope of this notebook, we will explore the KNN approach."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Vector databases"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Vector databases are highly specialized systems expertly designed for the efficient storage, retrieval, and management of high-dimensional vectors. Unlike traditional databases that limit themselves to scalar values like integers, vector databases excel in processing tasks that incorporate complex data types such as text, images, and audio mapped to multi-dimensional vector spaces, adeptly capturing relationships and similarities between data points. With the implementation of advanced indexing techniques and powerful similarity search algorithms, vector databases enable effective querying of large-scale datasets. This makes them essential tools for applications in natural language processing. \n",
    "\n",
    "One example represents the vector database pgvector, which we will later use for natural language processing tasks to store text embeddings in Programming Assignment 2. \n",
    "\n",
    "In the scope of this notebook, we will conduct simple KNN searches to obtain relevant information. First, we will start with the basics."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### System initialization"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In this tutorial we re-use PostgreSQL database Docker image we have presented in *Web crawling - basic tools* notebook. If you already have tested the showcase example and have not deleted the container *postgres-wier* with the database, you can start the Docker container as indicated below.\n",
    "\n",
    "\n",
    "\n",
    "<img src=\"https://i.ibb.co/DgCxJWdQ/docker-demo.png\" alt=\"docker-demo\" border=\"0\">\n",
    "\n",
    "\n",
    "\n",
    "Otherwise, follow next steps. \n",
    "\n",
    "First, prepare a file *database.sql*. The script will create a table with two rows:\n",
    "\n",
    "``` sql\n",
    "CREATE SCHEMA IF NOT EXISTS showcase;\n",
    "\n",
    "CREATE TABLE showcase.counters (\n",
    "    counter_id integer  NOT NULL,\n",
    "    value integer NOT NULL,\n",
    "    CONSTRAINT pk_counters PRIMARY KEY ( counter_id )\n",
    " );\n",
    "\n",
    "INSERT INTO showcase.counters VALUES (1,0), (2,0);\n",
    "```\n",
    "\n",
    "Go to an empty folder and save the script into a subfolder named *init-scripts*. Create another empty folder named *pgdata*.\n",
    "\n",
    "We run docker container using the following command. The command will name the container *postgresql-wier*, set username and password, map database files to folder *./pgdata* and initialization scripts to *./init-scripts*, map port 5432 to host machine (i.e. localhost) and run image *pgvector:pg16* in a detached mode. \n",
    "\n",
    "``` \n",
    "docker run --name postgresql-wier \\\n",
    "    -e POSTGRES_PASSWORD=SecretPassword \\\n",
    "    -e POSTGRES_USER=user \\\n",
    "    -e POSTGRES_DB=wier \\\n",
    "    -v $PWD/pgdata:/var/lib/postgresql/data \\\n",
    "    -v $PWD/init-scripts:/docker-entrypoint-initdb.d \\\n",
    "    -p 5432:5432 \\\n",
    "    -d pgvector/pgvector:pg16\n",
    "```\n",
    "\n",
    "If you use Command Prompt on Windows, the equivalent of the above command is as follows:\n",
    "\n",
    "``` \n",
    "docker run --name postgresql-wier ^\n",
    "    -e POSTGRES_PASSWORD=SecretPassword ^\n",
    "    -e POSTGRES_USER=user ^\n",
    "    -e POSTGRES_DB=wier ^\n",
    "    -v \"%CD%\\pgdata:/var/lib/postgresql/data\" ^\n",
    "    -v \"%CD%\\init-scripts:/docker-entrypoint-initdb.d\" ^\n",
    "    -p 5432:5432 ^\n",
    "    -d pgvector/pgvector:pg16\n",
    "```\n",
    "\n",
    "To check container's logs, run `docker logs -f postgresql-wier`.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Getting started"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We will enable the pgvector extension and insert a some example sentences in the database.  \n",
    "\n",
    "But first, do check that you use the correct image for your database, i.e., *pgvector/pgvector:pg16*.  \n",
    "\n",
    "Also, install the necessary dependencies."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%pip install pgvector\n",
    "%pip install sentence_transformers\n",
    "%pip install numpy"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In the next step, we will enable the pgvector extension in your PostgreSQL database (do this once in each database where you want to use it) with the following SQL statement:\n",
    "```sql\n",
    "CREATE EXTENSION IF NOT EXISTS vector\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pgvector.psycopg import register_vector\n",
    "import psycopg\n",
    "\n",
    "#connect to db\n",
    "conn = psycopg.connect(host=\"localhost\", dbname='wier', autocommit=True, password='SecretPassword', user='user')\n",
    "\n",
    "#enable `vector` extension if not already enabled\n",
    "conn.execute('CREATE EXTENSION IF NOT EXISTS vector')\n",
    "register_vector(conn)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now, we will create two new tables *showcase.vector_demo* and *showcase.vector_demo2*, for storing embeddings. Both tables have similar column definitions (except for embedding vector size):\n",
    "- **id**: primary key (unique for each sentence)\n",
    "- **sentence**: text (content) of the sentence\n",
    "- **embedding**: vector representation of the sentence.  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#delete tables vector_demo and vector_demo2 from the db if they exist\n",
    "conn.execute('DROP TABLE IF EXISTS showcase.vector_demo')\n",
    "conn.execute('DROP TABLE IF EXISTS showcase.vector_demo2')\n",
    "\n",
    "#create tables vector_demo and vector_demo2 with columns id, content, and embedding\n",
    "conn.execute('CREATE TABLE showcase.vector_demo (id bigserial PRIMARY KEY, sentence text, embedding vector(384))')\n",
    "conn.execute('CREATE TABLE showcase.vector_demo2 (id bigserial PRIMARY KEY, sentence text, embedding vector(768))')\n",
    "\n",
    "conn.close()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Next, we will define a list of sentences and calculate their embeddings using two different models from the [SentenceTransformer](https://sbert.net/) library, i.e.\n",
    "- [all-MiniLM-L6-v2](https://huggingface.co/sentence-transformers/all-MiniLM-L6-v2)\n",
    "- [LaBSE](https://huggingface.co/sentence-transformers/LaBSE)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sentence_transformers import SentenceTransformer\n",
    "\n",
    "#define sentences to be stored in the database\n",
    "sentences = [\n",
    "    'The sun is shining',\n",
    "    'The sun shines with great brightness',\n",
    "    'The sun shines',\n",
    "    'The sun provides light and energy',\n",
    "    'The sun shines brightly in the clear sky',\n",
    "    'The sun shines brightly, warming the earth and providing light',\n",
    "    'The clouds are covering the sky'    \n",
    "]\n",
    "\n",
    "#load SentenceTransformer model and generate embeddings\n",
    "model = SentenceTransformer('sentence-transformers/all-MiniLM-L6-v2')\n",
    "embeddings = model.encode(sentences)\n",
    "\n",
    "model2 = SentenceTransformer('sentence-transformers/LaBSE')\n",
    "embeddings2 = model2.encode(sentences)\n",
    "\n",
    "print('Size of embedding: ' + str(embeddings.shape))\n",
    "print('Size of embedding1: ' + str(embeddings2.shape))\n",
    "\n",
    "print('\\n\\n\\nembedding:\\n ')\n",
    "print(embeddings)\n",
    "\n",
    "print('\\n\\n\\nembedding 2:\\n ')\n",
    "print(embeddings2)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "After, we will store sentences in the database."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#print values in stored in the table showcase.vector_demo2\n",
    "def print_db_values():\n",
    "\n",
    "    \"\"\"\n",
    "    Print sentences and corresponding embeddings in the table showcase.vector_demo2.\n",
    "    \"\"\"\n",
    "    conn = psycopg.connect(host=\"localhost\", dbname='wier', autocommit=True, password='SecretPassword', user='user')\n",
    "\n",
    "    retVal = []\n",
    "    print(\"\\nValues in the vector_demo2 table:\")\n",
    "    cur = conn.cursor()\n",
    "    cur.execute(\"SELECT id, sentence, embedding FROM showcase.vector_demo2 ORDER BY id\")\n",
    "    for id, sentence, embedding in cur.fetchall():\n",
    "        print(f\"\\Id: {id},   Sentence: {sentence},   Embedding: {embedding}\")\n",
    "        retVal.append({id: (sentence, embedding)})\n",
    "    cur.close()\n",
    "    conn.close()\n",
    "\n",
    "    #return retVal\n",
    "\n",
    "#insert a list of sentences and corresponding embeddings in the table showcase.vector_demo\n",
    "def insert_db_sentences(sentences, embeddings):\n",
    "    \"\"\"\n",
    "    Insert a list of sentences and corresponding embeddings in the table showcase.vector_demo.\n",
    "\n",
    "    Parameters\n",
    "    - sentences: A list of sentences to be inserted in the vector_demo table.\n",
    "    - embeddings:  Embeddings to be inserted in the vector_demo table.\n",
    "    \"\"\"\n",
    "    conn = psycopg.connect(host=\"localhost\", dbname='wier', autocommit=True, password='SecretPassword', user='user')\n",
    "    cur = conn.cursor() \n",
    "    for sentence, embedding in zip(sentences, embeddings):\n",
    "        embedding = embedding.tolist() #convert numpy array to python lists for compatibility with PostgreSQL\n",
    "        cur.execute('INSERT INTO showcase.vector_demo (sentence, embedding) VALUES (%s, %s)', (sentence, embedding))\n",
    "    cur.close()\n",
    "    conn.close()\n",
    "\n",
    "\n",
    "#insert a list of sentences and corresponding embeddings in the table showcase.vector_demo2\n",
    "def insert_db_sentences2(sentences, embeddings):\n",
    "    \"\"\"\n",
    "    Insert a list of sentences and corresponding embeddings in the table showcase.vector_demo2.\n",
    "\n",
    "    Parameters\n",
    "    - sentences: A list of sentences to be inserted in the vector_demo2 table.\n",
    "    - embeddings:  Embeddings to be inserted in the vector_demo2 table.\n",
    "    \"\"\"\n",
    "    conn = psycopg.connect(host=\"localhost\", dbname='wier', autocommit=True, password='SecretPassword', user='user')\n",
    "    cur = conn.cursor() \n",
    "    for sentence, embedding in zip(sentences, embeddings):\n",
    "        embedding = embedding.tolist() #convert numpy array to python lists for compatibility with PostgreSQL\n",
    "        cur.execute('INSERT INTO showcase.vector_demo2 (sentence, embedding) VALUES (%s, %s)', (sentence, embedding))\n",
    "    cur.close()\n",
    "    conn.close()\n",
    "\n",
    "\n",
    "insert_db_sentences(sentences, embeddings)\n",
    "insert_db_sentences2(sentences, embeddings2)\n",
    "print_db_values()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Distance functions supported in pgvector database"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Pgvector supports the following distance functions are:\n",
    "\n",
    "- L2 distance (**`<->`**)\n",
    "- (Negative) inner product (**`<#>`**)\n",
    "- Cosine distance (**`<=>`**)\n",
    "- L1 distance(**`<+>`**)\n",
    "- Hamming distance (**`<~>`**): used for binary vectors\n",
    "- Jaccard distance (**`<%>`**): used for binary vectors"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "####  L2 distance (`<->`)  \n",
    "\n",
    "Let vectors $x$, $y \\in \\mathbb{R}^n$. L2 distance (the Euclidean distance) $d_{L2}(x, y)$ is defined as\n",
    "$d_{L2}(x, y) = \\sqrt{\\sum_{i=1}^{n} (x_i - y_i)^2}$.\n",
    "\n",
    "#### (Negative) inner product (`<#>`)\n",
    "The negative inner product of the dot product between two vectors. The inner product measures similarity, so negating it turns it into a pseudo-distance measure. For vectors $x$, $y \\in \\mathbb{R}^n$, the negative inner product $d_{\\text{inner}}(x, y)$ is defined as\n",
    "$d_{\\text{inner}}(x, y) = - (x \\cdot y)$.\n",
    "\n",
    "#### Cosine distance (`<=>`)  \n",
    "Cosine distance measures the angle between two vectors in a high-dimensional space. Let vectors $x$, $y \\in \\mathbb{R}^n$. The cosine distance is defined as\n",
    "$d_{\\cos}(x, y) = 1 - \\frac{x \\cdot y}{ \\lVert x \\rVert \\lVert y \\rVert}$\n",
    "For cosine similarity, use $1 - d_{\\cos}(x, y)$.\n",
    "\n",
    "####  L1 Distance (`<+>`)  \n",
    "L1 distance is also known as Manhattan distance. Let vectors $x$, $y \\in \\mathbb{R}^n$. The Manhattan distance is defined as:\n",
    "$d_{L1}(x,y) = \\sum_{i=1}^{n} |x_i - y_i|$\n",
    "\n",
    "#### Hamming Distance (`<~>`)  \n",
    "Hamming distance is used for binary vectors, as it counts the number of positions at which the corresponding elements differ. For vectors $x$, $y \\in \\{0, 1\\}^n$, the Hamming distance is defined as \n",
    "$d_{Hamming}(x,y) = \\sum_{i=1}^{n} |x_i - y_i|$. Obviously, $x_i - y_i \\in \\{0, 1\\}$ for all $i \\in {1, ..., n}$.\n",
    "  \n",
    "\n",
    "#### Jaccard Distance (`<%>`)  \n",
    "Jaccard distance is used for binary vectors. It is derived from the Jaccard similarity. For vectors $x$, $y \\in \\{0, 1\\}^n$, the Jaccard distance is defined as:\n",
    "$d_{Jaccard}(x,y) = 1 - \\frac{|x \\cap y |}{| x \\cup y |}$, where $ | x \\cap y | $ is the number of positions where both vectors have 1s, and $ | x \\cup y | $ is the number of positions where at least one vector has a 1.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Querying pgvector database"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now that we have listed all the distance functions supported by the PNG vector database, we can showcase the differences in results using the above metrics. First, we will define functions for querying the database using the above distances for non-binary vectors."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#query using L2 distance\n",
    "def query_db_L2(query, model_name, table_name):\n",
    "    \"\"\"\n",
    "    The query_db_L2 function retrieves the top 5 most similar sentences from a pgvector database based on L2 (Euclidean) distance. \n",
    "    It uses a pre-trained SentenceTransformer model to encode the input query and then searches for the closest embeddings stored in the database.\n",
    "\n",
    "    Parameters\n",
    "    - query (str): The input text query to be searched.\n",
    "    - model_name (str): The name of the SentenceTransformer model to be used for encoding the query.\n",
    "    - table_name (str): The name of the table containing the stored sentence embeddings. Possible options are showcase.vector_demo and showcase.vector_demo2\n",
    "    \"\"\"\n",
    "    \n",
    "    #download the model\n",
    "    model = SentenceTransformer(model_name)\n",
    "\n",
    "    #calculate embedding for the query\n",
    "    query_embedding = model.encode(query).tolist()  \n",
    "\n",
    "    conn = psycopg.connect(host=\"localhost\", dbname='wier', autocommit=True, password='SecretPassword', user='user')\n",
    "    cur = conn.cursor() \n",
    "\n",
    "    # execute the query to fetch the top 5 most similar sentences based on L2 distance\n",
    "    result = cur.execute(\n",
    "        'SELECT sentence, (embedding <-> %s::vector) AS distance '\n",
    "        'FROM ' + table_name + ' '\n",
    "        'ORDER BY embedding <-> %s::vector '\n",
    "        'LIMIT 5',\n",
    "        (query_embedding, query_embedding)  # pass the embedding twice, once for ordering and once for calculation\n",
    "    ).fetchall()\n",
    "    cur.close()\n",
    "    conn.close()\n",
    "    return result\n",
    "\n",
    "#query using L1 distance\n",
    "def query_db_L1(query, model_name, table_name):\n",
    "    \"\"\"\n",
    "    The query_db_L1 function retrieves the top 5 most similar sentences from a pgvector database based on L1 (Manhattan) distance. \n",
    "    It uses a pre-trained SentenceTransformer model to encode the input query and then searches for the closest embeddings stored in the database.\n",
    "\n",
    "    Parameters\n",
    "    - query (str): The input text query to be searched.\n",
    "    - model_name (str): The name of the SentenceTransformer model to be used for encoding the query.\n",
    "    - table_name (str): The name of the table containing the stored sentence embeddings. Possible options are showcase.vector_demo and showcase.vector_demo2\n",
    "    \"\"\"\n",
    "    \n",
    "    #download the model\n",
    "    model = SentenceTransformer(model_name)\n",
    "\n",
    "    #calculate embedding for the query\n",
    "    query_embedding = model.encode(query).tolist()  \n",
    "\n",
    "    conn = psycopg.connect(host=\"localhost\", dbname='wier', autocommit=True, password='SecretPassword', user='user')\n",
    "    cur = conn.cursor() \n",
    "\n",
    "    # execute the query to fetch the top 5 most similar sentences based on L1 distance\n",
    "    result = cur.execute(\n",
    "        'SELECT sentence, (embedding <+> %s::vector) AS distance '\n",
    "        'FROM ' + table_name + ' '\n",
    "        'ORDER BY embedding <+> %s::vector '\n",
    "        'LIMIT 5',\n",
    "        (query_embedding, query_embedding)  # pass the embedding twice, once for ordering and once for calculation\n",
    "    ).fetchall()\n",
    "    cur.close()\n",
    "    conn.close()\n",
    "    return result\n",
    "\n",
    "\n",
    "#query using cosine distance\n",
    "def query_db_cosine(query, model_name, table_name):\n",
    "    \"\"\"\n",
    "    The query_db_cosine function retrieves the top 5 most similar sentences from a pgvector database based on cosine distance. \n",
    "    It uses a pre-trained SentenceTransformer model to encode the input query and then searches for the closest embeddings stored in the database.\n",
    "\n",
    "    Parameters\n",
    "    - query (str): The input text query to be searched.\n",
    "    - model_name (str): The name of the SentenceTransformer model to be used for encoding the query.\n",
    "    - table_name (str): The name of the table containing the stored sentence embeddings. Possible options are showcase.vector_demo and showcase.vector_demo2\n",
    "    \"\"\"\n",
    "    \n",
    "    #download the model\n",
    "    model = SentenceTransformer(model_name)\n",
    "\n",
    "    #calculate embedding for the query\n",
    "    query_embedding = model.encode(query).tolist()  \n",
    "\n",
    "    conn = psycopg.connect(host=\"localhost\", dbname='wier', autocommit=True, password='SecretPassword', user='user')\n",
    "    cur = conn.cursor() \n",
    "\n",
    "    # execute the query to fetch the top 5 most similar sentences based on cosine distance\n",
    "    result = cur.execute(\n",
    "        'SELECT sentence, 1 - (embedding <=> %s::vector) AS distance '\n",
    "        'FROM ' + table_name + ' '\n",
    "        'ORDER BY embedding <=> %s::vector '\n",
    "        'LIMIT 5',\n",
    "        (query_embedding, query_embedding)  # pass the embedding twice, once for ordering and once for calculation\n",
    "    ).fetchall()\n",
    "    cur.close()\n",
    "    conn.close()\n",
    "    return result\n",
    "\n",
    "#query using negative inner product\n",
    "def query_db_inner(query, model_name, table_name):\n",
    "    \"\"\"\n",
    "    The query_db_inner function retrieves the top 5 most similar sentences from a pgvector database based on (negative) inner product. \n",
    "    It uses a pre-trained SentenceTransformer model to encode the input query and then searches for the closest embeddings stored in the database.\n",
    "\n",
    "    Parameters\n",
    "    - query (str): The input text query to be searched.\n",
    "    - model_name (str): The name of the SentenceTransformer model to be used for encoding the query.\n",
    "    - table_name (str): The name of the table containing the stored sentence embeddings. Possible options are showcase.vector_demo and showcase.vector_demo2\n",
    "    \"\"\"\n",
    "    \n",
    "    #download the model\n",
    "    model = SentenceTransformer(model_name)\n",
    "\n",
    "    #calculate embedding for the query\n",
    "    query_embedding = model.encode(query).tolist()  \n",
    "\n",
    "    conn = psycopg.connect(host=\"localhost\", dbname='wier', autocommit=True, password='SecretPassword', user='user')\n",
    "    cur = conn.cursor() \n",
    "\n",
    "    # execute the query to fetch the top 5 most similar sentences based negative inner product\n",
    "    result = cur.execute(\n",
    "        'SELECT sentence, -(embedding <#> %s::vector) AS distance '\n",
    "        'FROM ' + table_name + ' '\n",
    "        'ORDER BY embedding <#> %s::vector '\n",
    "        'LIMIT 5',\n",
    "        (query_embedding, query_embedding)  # pass the embedding twice, once for ordering and once for calculation\n",
    "    ).fetchall()\n",
    "    cur.close()\n",
    "    conn.close()\n",
    "    return result\n",
    "\n",
    "\n",
    "#print results\n",
    "def print_results(result):\n",
    "    \"\"\"\n",
    "    This function displays the results.\n",
    "\n",
    "    Parameters\n",
    "    - result: a list of tuples including sentence and embedding values\n",
    "    \"\"\"\n",
    "    for i,(sentence, distance) in enumerate(result, start=1):\n",
    "        print(f\"{i}. {sentence} {distance}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### 1. Identical sentence (we query the same vector)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The query sentence \"The sun is shining\" is already stored in our database. For this reason, we obtain the perfect match across all distances."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_name = 'sentence-transformers/all-MiniLM-L6-v2'\n",
    "query = 'The sun is shining'\n",
    "table_name = 'showcase.vector_demo'\n",
    "\n",
    "#L2 distance\n",
    "resultL2 = query_db_L2(query, model_name, table_name)\n",
    "print(\"\\n**Top 5 similar sentences using L2 distance:**\\n\")\n",
    "print_results(resultL2)\n",
    "\n",
    "#L1 distance\n",
    "resultL1 = query_db_L1(query, model_name, table_name)\n",
    "print(\"\\n**Top 5 similar sentences using L1 (Manhattan) distance:**\\n\")\n",
    "print_results(resultL1)\n",
    "\n",
    "#cosine distance\n",
    "resultC = query_db_cosine(query, model_name, table_name)\n",
    "print(\"\\n**Top 5 similar sentences using cosine distance:**\\n\")\n",
    "print_results(resultC)\n",
    "\n",
    "#negative inner product\n",
    "result_inner_product = query_db_inner(query, model_name, table_name)\n",
    "print(\"\\n**Top 5 similar sentences using negative inner product:**\\n\")\n",
    "print_results(result_inner_product)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### 2. Synonymous sentence (different wording, same meaning)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In this example, the query sentence is \"The sun is shining brightly\". We do not have the exact match stored in the database. Even though sentences have different wordings, they express the same idea, allowing us to see how cosine distance and inner product treat them as similar. In contrast, L2 and L1 distances show slight differences. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_name = 'sentence-transformers/all-MiniLM-L6-v2'\n",
    "query = 'The sun is shining brightly'\n",
    "table_name = 'showcase.vector_demo'\n",
    "\n",
    "#L2 distance\n",
    "resultL2 = query_db_L2(query, model_name, table_name)\n",
    "print(\"\\n**Top 5 similar sentences using L2 distance:**\\n\")\n",
    "print_results(resultL2)\n",
    "\n",
    "#L1 distance\n",
    "resultL1 = query_db_L1(query, model_name, table_name)\n",
    "print(\"\\n**Top 5 similar sentences using L1 (Manhattan) distance:**\\n\")\n",
    "print_results(resultL1)\n",
    "\n",
    "#cosine distance\n",
    "resultC = query_db_cosine(query, model_name, table_name)\n",
    "print(\"\\n**Top 5 similar sentences using cosine distance:**\\n\")\n",
    "print_results(resultC)\n",
    "\n",
    "#negative inner product\n",
    "result_inner_product = query_db_inner(query, model_name, table_name)\n",
    "print(\"\\n**Top 5 similar sentences using negative inner product:**\\n\")\n",
    "print_results(result_inner_product)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 3. Contrasting sentence (antonyms)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "If our query is \"The moon is glowing\", one can see that this sentence and sentences in the database are semantically opposite (e.g., sun versus moon, shining versus glowing). For this reason, the distance metrics show a higher degree of dissimilarity for them, particularly L2 and L1 distances."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "query = 'The moon is glowing'\n",
    "model_name = 'sentence-transformers/all-MiniLM-L6-v2'\n",
    "table_name = 'showcase.vector_demo'\n",
    "\n",
    "#L2 distance\n",
    "resultL2 = query_db_L2(query, model_name, table_name)\n",
    "print(\"\\n**Top 5 similar sentences using L2 distance:**\\n\")\n",
    "print_results(resultL2)\n",
    "\n",
    "#L1 distance\n",
    "resultL1 = query_db_L1(query, model_name, table_name)\n",
    "print(\"\\n**Top 5 similar sentences using L1 (Manhattan) distance:**\\n\")\n",
    "print_results(resultL1)\n",
    "\n",
    "#cosine distance\n",
    "resultC = query_db_cosine(query, model_name, table_name)\n",
    "print(\"\\n**Top 5 similar sentences using cosine distance:**\\n\")\n",
    "print_results(resultC)\n",
    "\n",
    "#negative inner product\n",
    "result_inner_product = query_db_inner(query, model_name, table_name)\n",
    "print(\"\\n**Top 5 similar sentences using negative inner product:**\\n\")\n",
    "print_results(result_inner_product)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 4. Short versus long sentences (different sentence lenghts)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The short sentences compared to the long ones will help see how cosine distance and inner product are slightly affected by the amount of content. However, overall, we still obtain similar results across all distances."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "query = 'The sun shines in the sky, a gentle breeze rustles the leaves and birds chirp in harmony, welcoming a new day filled with endless possibilities'\n",
    "model_name = 'sentence-transformers/all-MiniLM-L6-v2'\n",
    "table_name = 'showcase.vector_demo'\n",
    "\n",
    "#L2 distance\n",
    "resultL2 = query_db_L2(query, model_name, table_name)\n",
    "print(\"\\n**Top 5 similar sentences using L2 distance:**\\n\")\n",
    "print_results(resultL2)\n",
    "\n",
    "#L1 distance\n",
    "resultL1 = query_db_L1(query, model_name, table_name)\n",
    "print(\"\\n**Top 5 similar sentences using L1 (Manhattan) distance:**\\n\")\n",
    "print_results(resultL1)\n",
    "\n",
    "#cosine distance\n",
    "resultC = query_db_cosine(query, model_name, table_name)\n",
    "print(\"\\n**Top 5 similar sentences using cosine distance:**\\n\")\n",
    "print_results(resultC)\n",
    "\n",
    "#negative inner product\n",
    "result_inner_product = query_db_inner(query, model_name, table_name)\n",
    "print(\"\\n**Top 5 similar sentences using negative inner product:**\\n\")\n",
    "print_results(result_inner_product)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 5. Different topics"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Our query sentence includes an entirely different topic. As a consequence, the obtained results indicate a higher degree of dissimilarity."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "query = 'Albert Einstein'\n",
    "model_name = 'sentence-transformers/all-MiniLM-L6-v2'\n",
    "table_name = 'showcase.vector_demo'\n",
    "\n",
    "#L2 distance\n",
    "resultL2 = query_db_L2(query, model_name, table_name)\n",
    "print(\"\\n**Top 5 similar sentences using L2 distance:**\\n\")\n",
    "print_results(resultL2)\n",
    "\n",
    "#L1 distance\n",
    "resultL1 = query_db_L1(query, model_name, table_name)\n",
    "print(\"\\n**Top 5 similar sentences using L1 (Manhattan) distance:**\\n\")\n",
    "print_results(resultL1)\n",
    "\n",
    "#cosine distance\n",
    "resultC = query_db_cosine(query, model_name, table_name)\n",
    "print(\"\\n**Top 5 similar sentences using cosine distance:**\\n\")\n",
    "print_results(resultC)\n",
    "\n",
    "#negative inner product\n",
    "result_inner_product = query_db_inner(query, model_name, table_name)\n",
    "print(\"\\n**Top 5 similar sentences using negative inner product:**\\n\")\n",
    "print_results(result_inner_product)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 6. Minor changes (similar meaning but slight rewording)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The query \"Shining is the sun\" has a similar meaning as the sentence \"The sun is shining\", which is already included in the database. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "query = 'Shining is the sun'\n",
    "model_name = 'sentence-transformers/all-MiniLM-L6-v2'\n",
    "table_name = 'showcase.vector_demo'\n",
    "\n",
    "#L2 distance\n",
    "resultL2 = query_db_L2(query, model_name, table_name)\n",
    "print(\"\\n**Top 5 similar sentences using L2 distance:**\\n\")\n",
    "print_results(resultL2)\n",
    "\n",
    "#L1 distance\n",
    "resultL1 = query_db_L1(query, model_name, table_name)\n",
    "print(\"\\n**Top 5 similar sentences using L1 (Manhattan) distance:**\\n\")\n",
    "print_results(resultL1)\n",
    "\n",
    "#cosine distance\n",
    "resultC = query_db_cosine(query, model_name, table_name)\n",
    "print(\"\\n**Top 5 similar sentences using cosine distance:**\\n\")\n",
    "print_results(resultC)\n",
    "\n",
    "#negative inner product\n",
    "result_inner_product = query_db_inner(query, model_name, table_name)\n",
    "print(\"\\n**Top 5 similar sentences using negative inner product:**\\n\")\n",
    "print_results(result_inner_product)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 7.  What about Slovene?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "So far, we have tested several examples in English. Let's try one example sentence in Slovene, e.g. \"Sonce sije svetlo na jasnem nebu\". Its translation into English is \"The sun is shining brightly in the clear sky\". We already have this sentence stored in the database. \n",
    "\n",
    "`How will a query in Slovene impact the results?`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "query = 'Sonce sije svetlo na jasnem nebu' #The sun is shining brightly in the clear sky\n",
    "model_name = 'sentence-transformers/all-MiniLM-L6-v2'\n",
    "table_name = 'showcase.vector_demo'\n",
    "\n",
    "#L2 distance\n",
    "resultL2 = query_db_L2(query, model_name, table_name)\n",
    "print(\"\\n**Top 5 similar sentences using L2 distance:**\\n\")\n",
    "print_results(resultL2)\n",
    "\n",
    "#L1 distance\n",
    "resultL1 = query_db_L1(query, model_name, table_name)\n",
    "print(\"\\n**Top 5 similar sentences using L1 (Manhattan) distance:**\\n\")\n",
    "print_results(resultL1)\n",
    "\n",
    "#cosine distance\n",
    "resultC = query_db_cosine(query, model_name, table_name)\n",
    "print(\"\\n**Top 5 similar sentences using cosine distance:**\\n\")\n",
    "print_results(resultC)\n",
    "\n",
    "#negative inner product\n",
    "result_inner_product = query_db_inner(query, model_name, table_name)\n",
    "print(\"\\n**Top 5 similar sentences using negative inner product:**\\n\")\n",
    "print_results(result_inner_product)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can see that the *all-MiniLM-L6-v2* model yields results that are not as expected due to the sentence \"The sun shines\" being the best option for all distances.\n",
    "\n",
    "Some models, like the *LaBSE* model, are multilingual. Such models support different languages. Specifically, the *LaBSE* model supports both Slovene and English. Let's see how this model will perform."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "query = 'Sonce sije svetlo na jasnem nebu' #The sun is shining brightly in the clear sky\n",
    "model_name = 'sentence-transformers/LaBSE'\n",
    "table_name = 'showcase.vector_demo2'\n",
    "\n",
    "#L2 distance\n",
    "resultL2 = query_db_L2(query, model_name, table_name)\n",
    "print(\"\\n**Top 5 similar sentences using L2 distance:**\\n\")\n",
    "print_results(resultL2)\n",
    "\n",
    "#L1 distance\n",
    "resultL1 = query_db_L1(query, model_name, table_name)\n",
    "print(\"\\n**Top 5 similar sentences using L1 (Manhattan) distance:**\\n\")\n",
    "print_results(resultL1)\n",
    "\n",
    "#cosine distance\n",
    "resultC = query_db_cosine(query, model_name, table_name)\n",
    "print(\"\\n**Top 5 similar sentences using cosine distance:**\\n\")\n",
    "print_results(resultC)\n",
    "\n",
    "#negative inner product\n",
    "result_inner_product = query_db_inner(query, model_name, table_name)\n",
    "print(\"\\n**Top 5 similar sentences using negative inner product:**\\n\")\n",
    "print_results(result_inner_product)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This model yields the expected results. \n",
    "\n",
    "In this tutorial, we have demonstrated that the model used for calculating embeddings plays a crucial role in vector retrieval. So far, we have utilized the KNN approach to search the database with non-binary vectors. \n",
    "\n",
    "You can test querying the database using Hamming and Jaccard distances for practice. To achieve this, you must select a model that calculates embeddings as binary vectors, create a new table for storing the embeddings and implement the code for querying using the Hamming and Jaccard distances. You can help with [this example](https://github.com/pgvector/pgvector-python/blob/master/examples/cohere/example.py)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## References\n",
    "\n",
    "- [Understanding Vector databases.](https://learn.microsoft.com/en-us/data-engineering/playbook/solutions/vector-database/)\n",
    "- [Book Vector Spaces First: Introduction to Linear Algebra](https://ruor.uottawa.ca/items/f66a4ede-e276-486c-9067-9621d5347440)\n",
    "- [Vector database pgvector.](https://github.com/pgvector/pgvector)\n",
    "- [pgvector-python](https://github.com/pgvector/pgvector-python)\n",
    "- [Sentence transformers.](https://sbert.net/)\n",
    "- [Model all-MiniLM-L6-v2](https://huggingface.co/sentence-transformers/all-MiniLM-L6-v2)\n",
    "- [Model LaBSE](https://huggingface.co/sentence-transformers/LaBSE)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
